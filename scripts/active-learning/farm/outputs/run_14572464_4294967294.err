Unloading openmpi/4.1.5
Unloading slurm/23.02.7
Loading slurm/23.02.7
Loading openmpi/4.1.5
INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.11 (you have 1.4.8). Upgrade using: pip install --upgrade albumentations
INFO: [rank: 0] Seed set to 42
INFO:lightning.fabric.utilities.seed:[rank: 0] Seed set to 42
INFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /tmp/tmp2i7kwwh0
INFO:torch.distributed.nn.jit.instantiator:Writing /tmp/tmp2i7kwwh0/_remote_module_non_scriptable.py
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python pascal_detector.py --root_dir /group/jmearlesgrp/dat ...
INFO: GPU available: True (cuda), used: True
INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True
INFO: TPU available: False, using: 0 TPU cores
INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores
INFO: IPU available: False, using: 0 IPUs
INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs
INFO: HPU available: False, using: 0 HPUs
INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs
wandb: Currently logged in as: earlranario (paibl). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/entropy_cluster/pascal_entropy_cluster_cold-start_0717/wandb/run-20240717_134048-lsu7os8g
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pascal_entropy_cluster_cold-start_0717_chunk-1
wandb: â­ï¸ View project at https://wandb.ai/paibl/active-learning
wandb: ğŸš€ View run at https://wandb.ai/paibl/active-learning/runs/lsu7os8g
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: 
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
INFO:lightning.pytorch.callbacks.model_summary:
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (48) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 24. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 15. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
INFO: `Trainer.fit` stopped: `max_epochs=50` reached.
INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=50` reached.
INFO: Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/entropy_cluster/pascal_entropy_cluster_cold-start_0717/active-learning/lsu7os8g/checkpoints/epoch=49-step=2400.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/entropy_cluster/pascal_entropy_cluster_cold-start_0717/active-learning/lsu7os8g/checkpoints/epoch=49-step=2400.ckpt
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/entropy_cluster/pascal_entropy_cluster_cold-start_0717/active-learning/lsu7os8g/checkpoints/epoch=49-step=2400.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/entropy_cluster/pascal_entropy_cluster_cold-start_0717/active-learning/lsu7os8g/checkpoints/epoch=49-step=2400.ckpt
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
wandb: - 0.007 MB of 0.007 MB uploadedwandb: \ 0.007 MB of 0.020 MB uploadedwandb: | 0.010 MB of 0.020 MB uploadedwandb: / 0.010 MB of 0.020 MB uploadedwandb: - 0.010 MB of 0.020 MB uploadedwandb: \ 0.010 MB of 0.020 MB uploadedwandb: | 0.010 MB of 0.020 MB uploadedwandb: / 0.020 MB of 0.020 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:             lr-Adam â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            test_map â–
wandb:         test_map_50 â–
wandb:         test_map_75 â–
wandb:      test_map_large â–
wandb:     test_map_medium â–
wandb:  test_map_per_class â–
wandb:      test_map_small â–
wandb:           test_size â–
wandb:    train_loss_epoch â–ˆâ–†â–†â–†â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     train_loss_step â–†â–†â–‡â–ˆâ–†â–‡â–‡â–†â–…â–†â–„â–…â–ƒâ–„â–„â–ƒâ–ƒâ–ƒâ–„â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–‚â–â–â–‚â–â–
wandb:          train_size â–
wandb: trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 50
wandb:             lr-Adam 0.0002
wandb:            test_map 0.09294
wandb:         test_map_50 0.22749
wandb:         test_map_75 0.05725
wandb:      test_map_large 0.11342
wandb:     test_map_medium 0.03485
wandb:  test_map_per_class -1.0
wandb:      test_map_small 0.01108
wandb:           test_size 5823
wandb:    train_loss_epoch 0.0767
wandb:     train_loss_step 0.09607
wandb:          train_size 1143
wandb: trainer/global_step 2400
wandb: 
wandb: ğŸš€ View run pascal_entropy_cluster_cold-start_0717_chunk-1 at: https://wandb.ai/paibl/active-learning/runs/lsu7os8g
wandb: â­ï¸ View project at: https://wandb.ai/paibl/active-learning
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/entropy_cluster/pascal_entropy_cluster_cold-start_0717/wandb/run-20240717_134048-lsu7os8g/logs
Traceback (most recent call last):
  File "/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/pascal_detector.py", line 107, in <module>
    main(
  File "/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/pascal_detector.py", line 41, in main
    train_dataset, test_dataset = us.sample(chunk=chunk, method=method, model=model, dm=PASCALDataModule, batch_size=batch_size)
  File "/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/sampling.py", line 60, in sample
    return self.entropy_cluster_based(chunk, model, dm, batch_size,
  File "/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/sampling.py", line 249, in entropy_cluster_based
    clusters[label].append(entropies[idx])
IndexError: list index out of range

-------------------------------------------------------------------------------
pascal_detector.py 107 <module>
main(

pascal_detector.py 41 main
train_dataset, test_dataset = us.sample(chunk=chunk, method=method, model=model, dm=PASCALDataModule, batch_size=batch_size)

sampling.py 60 sample
return self.entropy_cluster_based(chunk, model, dm, batch_size,

sampling.py 249 entropy_cluster_based
clusters[label].append(entropies[idx])

IndexError:
list index out of range
