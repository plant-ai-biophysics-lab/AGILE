Unloading openmpi/4.1.5
Unloading slurm/23.02.7
Loading slurm/23.02.7
Loading openmpi/4.1.5
INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.11 (you have 1.4.8). Upgrade using: pip install --upgrade albumentations
INFO: [rank: 0] Seed set to 42
INFO:lightning.fabric.utilities.seed:[rank: 0] Seed set to 42
INFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /tmp/tmpmy4uakdo
INFO:torch.distributed.nn.jit.instantiator:Writing /tmp/tmpmy4uakdo/_remote_module_non_scriptable.py
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python pascal_detector.py --root_dir /group/jmearlesgrp/dat ...
INFO: GPU available: True (cuda), used: True
INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True
INFO: TPU available: False, using: 0 TPU cores
INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores
INFO: IPU available: False, using: 0 IPUs
INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs
INFO: HPU available: False, using: 0 HPUs
INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs
wandb: Currently logged in as: earlranario (paibl). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_no-ckpt_0717/wandb/run-20240717_223014-x0mdkh0l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pascal_random_no-ckpt_0717_chunk-1
wandb: ⭐️ View project at https://wandb.ai/paibl/active-learning
wandb: 🚀 View run at https://wandb.ai/paibl/active-learning/runs/x0mdkh0l
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: 
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
INFO:lightning.pytorch.callbacks.model_summary:
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (24) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 24. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 19. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
INFO: `Trainer.fit` stopped: `max_epochs=100` reached.
INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.
INFO: Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_no-ckpt_0717/active-learning/x0mdkh0l/checkpoints/epoch=99-step=2400.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_no-ckpt_0717/active-learning/x0mdkh0l/checkpoints/epoch=99-step=2400.ckpt
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_no-ckpt_0717/active-learning/x0mdkh0l/checkpoints/epoch=99-step=2400.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_no-ckpt_0717/active-learning/x0mdkh0l/checkpoints/epoch=99-step=2400.ckpt
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
wandb: - 0.007 MB of 0.007 MB uploadedwandb: \ 0.007 MB of 0.007 MB uploadedwandb: | 0.007 MB of 0.007 MB uploadedwandb: / 0.012 MB of 0.025 MB uploadedwandb: - 0.025 MB of 0.025 MB uploadedwandb: \ 0.025 MB of 0.025 MB uploadedwandb: | 0.025 MB of 0.025 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:             lr-Adam ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            test_map ▁
wandb:         test_map_50 ▁
wandb:         test_map_75 ▁
wandb:      test_map_large ▁
wandb:     test_map_medium ▁
wandb:  test_map_per_class ▁
wandb:      test_map_small ▁
wandb:           test_size ▁
wandb:    train_loss_epoch █▅▅▅▅▅▄▄▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     train_loss_step ▆█▆█▇▆▅▃▄▅▃▂▃▃▂▃▂▂▂▂▂▁▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁
wandb:          train_size ▁
wandb: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████
wandb: 
wandb: Run summary:
wandb:               epoch 100
wandb:             lr-Adam 0.0002
wandb:            test_map 0.05383
wandb:         test_map_50 0.14194
wandb:         test_map_75 0.0285
wandb:      test_map_large 0.06506
wandb:     test_map_medium 0.01689
wandb:  test_map_per_class -1.0
wandb:      test_map_small 0.0048
wandb:           test_size 5823
wandb:    train_loss_epoch 0.05545
wandb:     train_loss_step 0.06312
wandb:          train_size 571
wandb: trainer/global_step 2400
wandb: 
wandb: 🚀 View run pascal_random_no-ckpt_0717_chunk-1 at: https://wandb.ai/paibl/active-learning/runs/x0mdkh0l
wandb: ⭐️ View project at: https://wandb.ai/paibl/active-learning
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_no-ckpt_0717/wandb/run-20240717_223014-x0mdkh0l/logs
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python pascal_detector.py --root_dir /group/jmearlesgrp/dat ...
INFO: GPU available: True (cuda), used: True
INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True
INFO: TPU available: False, using: 0 TPU cores
INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores
INFO: IPU available: False, using: 0 IPUs
INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs
INFO: HPU available: False, using: 0 HPUs
INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_no-ckpt_0717/wandb/run-20240718_000430-quvs212w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pascal_random_no-ckpt_0717_chunk-2
wandb: ⭐️ View project at https://wandb.ai/paibl/active-learning
wandb: 🚀 View run at https://wandb.ai/paibl/active-learning/runs/quvs212w
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: 
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
INFO:lightning.pytorch.callbacks.model_summary:
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (29) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 13. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
INFO: `Trainer.fit` stopped: `max_epochs=100` reached.
INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.
INFO: Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_no-ckpt_0717/active-learning/quvs212w/checkpoints/epoch=98-step=2871.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_no-ckpt_0717/active-learning/quvs212w/checkpoints/epoch=98-step=2871.ckpt
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_no-ckpt_0717/active-learning/quvs212w/checkpoints/epoch=98-step=2871.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_no-ckpt_0717/active-learning/quvs212w/checkpoints/epoch=98-step=2871.ckpt
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
wandb: - 0.007 MB of 0.007 MB uploadedwandb: \ 0.007 MB of 0.020 MB uploadedwandb: | 0.007 MB of 0.020 MB uploadedwandb: / 0.020 MB of 0.020 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:             lr-Adam ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            test_map ▁
wandb:         test_map_50 ▁
wandb:         test_map_75 ▁
wandb:      test_map_large ▁
wandb:     test_map_medium ▁
wandb:  test_map_per_class ▁
wandb:      test_map_small ▁
wandb:           test_size ▁
wandb:    train_loss_epoch █▆▅▅▅▅▄▄▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     train_loss_step ▇█▇█▆▆▇▆▄▄▃▃▃▃▃▂▃▂▂▂▂▁▂▂▂▁▁▁▂▁▂▁▁▂▁▁▁▂▁▁
wandb:          train_size ▁
wandb: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████
wandb: 
wandb: Run summary:
wandb:               epoch 100
wandb:             lr-Adam 0.0002
wandb:            test_map 0.06601
wandb:         test_map_50 0.16782
wandb:         test_map_75 0.03662
wandb:      test_map_large 0.07961
wandb:     test_map_medium 0.02558
wandb:  test_map_per_class -1.0
wandb:      test_map_small 0.00709
wandb:           test_size 5823
wandb:    train_loss_epoch 0.05179
wandb:     train_loss_step 0.02256
wandb:          train_size 685
wandb: trainer/global_step 2900
wandb: 
wandb: 🚀 View run pascal_random_no-ckpt_0717_chunk-2 at: https://wandb.ai/paibl/active-learning/runs/quvs212w
wandb: ⭐️ View project at: https://wandb.ai/paibl/active-learning
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_no-ckpt_0717/wandb/run-20240718_000430-quvs212w/logs
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python pascal_detector.py --root_dir /group/jmearlesgrp/dat ...
INFO: GPU available: True (cuda), used: True
INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True
INFO: TPU available: False, using: 0 TPU cores
INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores
INFO: IPU available: False, using: 0 IPUs
INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs
INFO: HPU available: False, using: 0 HPUs
INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_no-ckpt_0717/wandb/run-20240718_015542-mpnfaicp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pascal_random_no-ckpt_0717_chunk-3
wandb: ⭐️ View project at https://wandb.ai/paibl/active-learning
wandb: 🚀 View run at https://wandb.ai/paibl/active-learning/runs/mpnfaicp
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: 
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
INFO:lightning.pytorch.callbacks.model_summary:
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (34) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 7. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
INFO: `Trainer.fit` stopped: `max_epochs=100` reached.
INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.
INFO: Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_no-ckpt_0717/active-learning/mpnfaicp/checkpoints/epoch=99-step=3400.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_no-ckpt_0717/active-learning/mpnfaicp/checkpoints/epoch=99-step=3400.ckpt
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_no-ckpt_0717/active-learning/mpnfaicp/checkpoints/epoch=99-step=3400.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_no-ckpt_0717/active-learning/mpnfaicp/checkpoints/epoch=99-step=3400.ckpt
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
wandb: - 0.007 MB of 0.007 MB uploadedwandb: \ 0.007 MB of 0.020 MB uploadedwandb: | 0.007 MB of 0.020 MB uploadedwandb: / 0.020 MB of 0.020 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:             lr-Adam ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            test_map ▁
wandb:         test_map_50 ▁
wandb:         test_map_75 ▁
wandb:      test_map_large ▁
wandb:     test_map_medium ▁
wandb:  test_map_per_class ▁
wandb:      test_map_small ▁
wandb:           test_size ▁
wandb:    train_loss_epoch █▆▆▆▆▅▅▄▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     train_loss_step ▇▇▆█▇▆▆▅▄▃▃▂▃▂▂▂▁▂▂▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:          train_size ▁
wandb: trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb: 
wandb: Run summary:
wandb:               epoch 100
wandb:             lr-Adam 0.0002
wandb:            test_map 0.07222
wandb:         test_map_50 0.17084
wandb:         test_map_75 0.0494
wandb:      test_map_large 0.08797
wandb:     test_map_medium 0.02644
wandb:  test_map_per_class -1.0
wandb:      test_map_small 0.00593
wandb:           test_size 5823
wandb:    train_loss_epoch 0.04886
wandb:     train_loss_step 0.04664
wandb:          train_size 799
wandb: trainer/global_step 3400
wandb: 
wandb: 🚀 View run pascal_random_no-ckpt_0717_chunk-3 at: https://wandb.ai/paibl/active-learning/runs/mpnfaicp
wandb: ⭐️ View project at: https://wandb.ai/paibl/active-learning
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_no-ckpt_0717/wandb/run-20240718_015542-mpnfaicp/logs
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python pascal_detector.py --root_dir /group/jmearlesgrp/dat ...
INFO: GPU available: True (cuda), used: True
INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True
INFO: TPU available: False, using: 0 TPU cores
INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores
INFO: IPU available: False, using: 0 IPUs
INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs
INFO: HPU available: False, using: 0 HPUs
INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_no-ckpt_0717/wandb/run-20240718_040504-cx7k8p36
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pascal_random_no-ckpt_0717_chunk-4
wandb: ⭐️ View project at https://wandb.ai/paibl/active-learning
wandb: 🚀 View run at https://wandb.ai/paibl/active-learning/runs/cx7k8p36
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: 
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
INFO:lightning.pytorch.callbacks.model_summary:
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (39) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
INFO: `Trainer.fit` stopped: `max_epochs=100` reached.
INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.
INFO: Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_no-ckpt_0717/active-learning/cx7k8p36/checkpoints/epoch=88-step=3471.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_no-ckpt_0717/active-learning/cx7k8p36/checkpoints/epoch=88-step=3471.ckpt
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_no-ckpt_0717/active-learning/cx7k8p36/checkpoints/epoch=88-step=3471.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_no-ckpt_0717/active-learning/cx7k8p36/checkpoints/epoch=88-step=3471.ckpt
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
wandb: - 0.007 MB of 0.007 MB uploadedwandb: \ 0.007 MB of 0.007 MB uploadedwandb: | 0.007 MB of 0.021 MB uploadedwandb: / 0.007 MB of 0.021 MB uploadedwandb: - 0.021 MB of 0.021 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:             lr-Adam ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            test_map ▁
wandb:         test_map_50 ▁
wandb:         test_map_75 ▁
wandb:      test_map_large ▁
wandb:     test_map_medium ▁
wandb:  test_map_per_class ▁
wandb:      test_map_small ▁
wandb:           test_size ▁
wandb:    train_loss_epoch █▆▆▆▆▅▅▄▄▃▄▃▂▂▂▂▂▂▂▂▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     train_loss_step ▅▇▆▆█▇▅▅▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁
wandb:          train_size ▁
wandb: trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███
wandb: 
wandb: Run summary:
wandb:               epoch 100
wandb:             lr-Adam 0.0002
wandb:            test_map 0.0781
wandb:         test_map_50 0.1874
wandb:         test_map_75 0.05029
wandb:      test_map_large 0.09399
wandb:     test_map_medium 0.02803
wandb:  test_map_per_class -1.0
wandb:      test_map_small 0.00957
wandb:           test_size 5823
wandb:    train_loss_epoch 0.07225
wandb:     train_loss_step 0.02837
wandb:          train_size 913
wandb: trainer/global_step 3900
wandb: 
wandb: 🚀 View run pascal_random_no-ckpt_0717_chunk-4 at: https://wandb.ai/paibl/active-learning/runs/cx7k8p36
wandb: ⭐️ View project at: https://wandb.ai/paibl/active-learning
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_no-ckpt_0717/wandb/run-20240718_040504-cx7k8p36/logs
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python pascal_detector.py --root_dir /group/jmearlesgrp/dat ...
INFO: GPU available: True (cuda), used: True
INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True
INFO: TPU available: False, using: 0 TPU cores
INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores
INFO: IPU available: False, using: 0 IPUs
INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs
INFO: HPU available: False, using: 0 HPUs
INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_no-ckpt_0717/wandb/run-20240718_063326-bsu1kn6d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pascal_random_no-ckpt_0717_chunk-5
wandb: ⭐️ View project at https://wandb.ai/paibl/active-learning
wandb: 🚀 View run at https://wandb.ai/paibl/active-learning/runs/bsu1kn6d
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: 
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
INFO:lightning.pytorch.callbacks.model_summary:
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (43) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
INFO: `Trainer.fit` stopped: `max_epochs=100` reached.
INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.
INFO: Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_no-ckpt_0717/active-learning/bsu1kn6d/checkpoints/epoch=99-step=4300.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_no-ckpt_0717/active-learning/bsu1kn6d/checkpoints/epoch=99-step=4300.ckpt
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_no-ckpt_0717/active-learning/bsu1kn6d/checkpoints/epoch=99-step=4300.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_no-ckpt_0717/active-learning/bsu1kn6d/checkpoints/epoch=99-step=4300.ckpt
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
wandb: - 0.007 MB of 0.007 MB uploadedwandb: \ 0.007 MB of 0.021 MB uploadedwandb: | 0.010 MB of 0.021 MB uploadedwandb: / 0.010 MB of 0.021 MB uploadedwandb: - 0.010 MB of 0.021 MB uploadedwandb: \ 0.010 MB of 0.021 MB uploadedwandb: | 0.010 MB of 0.021 MB uploadedwandb: / 0.021 MB of 0.021 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:             lr-Adam ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            test_map ▁
wandb:         test_map_50 ▁
wandb:         test_map_75 ▁
wandb:      test_map_large ▁
wandb:     test_map_medium ▁
wandb:  test_map_per_class ▁
wandb:      test_map_small ▁
wandb:           test_size ▁
wandb:    train_loss_epoch █▆▆▆▅▅▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     train_loss_step ▆██▆▆▅▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:          train_size ▁
wandb: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb: 
wandb: Run summary:
wandb:               epoch 100
wandb:             lr-Adam 0.0002
wandb:            test_map 0.08434
wandb:         test_map_50 0.19465
wandb:         test_map_75 0.05855
wandb:      test_map_large 0.1009
wandb:     test_map_medium 0.03107
wandb:  test_map_per_class -1.0
wandb:      test_map_small 0.00548
wandb:           test_size 5823
wandb:    train_loss_epoch 0.04887
wandb:     train_loss_step 0.03326
wandb:          train_size 1027
wandb: trainer/global_step 4300
wandb: 
wandb: 🚀 View run pascal_random_no-ckpt_0717_chunk-5 at: https://wandb.ai/paibl/active-learning/runs/bsu1kn6d
wandb: ⭐️ View project at: https://wandb.ai/paibl/active-learning
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_no-ckpt_0717/wandb/run-20240718_063326-bsu1kn6d/logs
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python pascal_detector.py --root_dir /group/jmearlesgrp/dat ...
INFO: GPU available: True (cuda), used: True
INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True
INFO: TPU available: False, using: 0 TPU cores
INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores
INFO: IPU available: False, using: 0 IPUs
INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs
INFO: HPU available: False, using: 0 HPUs
INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_no-ckpt_0717/wandb/run-20240718_090724-0sxgk6xi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pascal_random_no-ckpt_0717_chunk-6
wandb: ⭐️ View project at https://wandb.ai/paibl/active-learning
wandb: 🚀 View run at https://wandb.ai/paibl/active-learning/runs/0sxgk6xi
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: 
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
INFO:lightning.pytorch.callbacks.model_summary:
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (48) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
INFO: `Trainer.fit` stopped: `max_epochs=100` reached.
INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.
INFO: Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_no-ckpt_0717/active-learning/0sxgk6xi/checkpoints/epoch=99-step=4800.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_no-ckpt_0717/active-learning/0sxgk6xi/checkpoints/epoch=99-step=4800.ckpt
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_no-ckpt_0717/active-learning/0sxgk6xi/checkpoints/epoch=99-step=4800.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_no-ckpt_0717/active-learning/0sxgk6xi/checkpoints/epoch=99-step=4800.ckpt
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
wandb: - 0.007 MB of 0.007 MB uploadedwandb: \ 0.007 MB of 0.007 MB uploadedwandb: | 0.007 MB of 0.022 MB uploadedwandb: / 0.007 MB of 0.022 MB uploadedwandb: - 0.022 MB of 0.022 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:             lr-Adam ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            test_map ▁
wandb:         test_map_50 ▁
wandb:         test_map_75 ▁
wandb:      test_map_large ▁
wandb:     test_map_medium ▁
wandb:  test_map_per_class ▁
wandb:      test_map_small ▁
wandb:           test_size ▁
wandb:    train_loss_epoch █▆▆▆▆▅▄▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     train_loss_step ▆▆█▇▅▅▄▃▅▄▃▂▃▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁
wandb:          train_size ▁
wandb: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb: 
wandb: Run summary:
wandb:               epoch 100
wandb:             lr-Adam 0.0002
wandb:            test_map 0.0947
wandb:         test_map_50 0.22309
wandb:         test_map_75 0.06173
wandb:      test_map_large 0.11491
wandb:     test_map_medium 0.03371
wandb:  test_map_per_class -1.0
wandb:      test_map_small 0.00619
wandb:           test_size 5823
wandb:    train_loss_epoch 0.04636
wandb:     train_loss_step 0.05849
wandb:          train_size 1141
wandb: trainer/global_step 4800
wandb: 
wandb: 🚀 View run pascal_random_no-ckpt_0717_chunk-6 at: https://wandb.ai/paibl/active-learning/runs/0sxgk6xi
wandb: ⭐️ View project at: https://wandb.ai/paibl/active-learning
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_no-ckpt_0717/wandb/run-20240718_090724-0sxgk6xi/logs
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python pascal_detector.py --root_dir /group/jmearlesgrp/dat ...
INFO: GPU available: True (cuda), used: True
INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True
INFO: TPU available: False, using: 0 TPU cores
INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores
INFO: IPU available: False, using: 0 IPUs
INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs
INFO: HPU available: False, using: 0 HPUs
INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_no-ckpt_0717/wandb/run-20240718_122009-0ltpywf4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pascal_random_no-ckpt_0717_chunk-7
wandb: ⭐️ View project at https://wandb.ai/paibl/active-learning
wandb: 🚀 View run at https://wandb.ai/paibl/active-learning/runs/0ltpywf4
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: 
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
INFO:lightning.pytorch.callbacks.model_summary:
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
