Unloading openmpi/4.1.5
Unloading slurm/23.02.7
Loading slurm/23.02.7
Loading openmpi/4.1.5
INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.11 (you have 1.4.8). Upgrade using: pip install --upgrade albumentations
INFO: [rank: 0] Seed set to 42
INFO:lightning.fabric.utilities.seed:[rank: 0] Seed set to 42
INFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /tmp/tmpk2z34xg4
INFO:torch.distributed.nn.jit.instantiator:Writing /tmp/tmpk2z34xg4/_remote_module_non_scriptable.py
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python pascal_detector.py --root_dir /group/jmearlesgrp/dat ...
INFO: GPU available: True (cuda), used: True
INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True
INFO: TPU available: False, using: 0 TPU cores
INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores
INFO: IPU available: False, using: 0 IPUs
INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs
INFO: HPU available: False, using: 0 HPUs
INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs
wandb: Currently logged in as: earlranario (paibl). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/BatchBALD/pascal_BatchBALD_cold-start_0717/wandb/run-20240717_134220-qq1n0h9q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pascal_BatchBALD_cold-start_0717_chunk-1
wandb: ⭐️ View project at https://wandb.ai/paibl/active-learning
wandb: 🚀 View run at https://wandb.ai/paibl/active-learning/runs/qq1n0h9q
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: 
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
INFO:lightning.pytorch.callbacks.model_summary:
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (48) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 24. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 15. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
INFO: `Trainer.fit` stopped: `max_epochs=50` reached.
INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=50` reached.
INFO: Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/BatchBALD/pascal_BatchBALD_cold-start_0717/active-learning/qq1n0h9q/checkpoints/epoch=49-step=2400.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/BatchBALD/pascal_BatchBALD_cold-start_0717/active-learning/qq1n0h9q/checkpoints/epoch=49-step=2400.ckpt
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/BatchBALD/pascal_BatchBALD_cold-start_0717/active-learning/qq1n0h9q/checkpoints/epoch=49-step=2400.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/BatchBALD/pascal_BatchBALD_cold-start_0717/active-learning/qq1n0h9q/checkpoints/epoch=49-step=2400.ckpt
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
wandb: - 0.007 MB of 0.007 MB uploadedwandb: \ 0.007 MB of 0.007 MB uploadedwandb: | 0.007 MB of 0.007 MB uploadedwandb: / 0.012 MB of 0.025 MB uploaded (0.003 MB deduped)wandb: - 0.012 MB of 0.025 MB uploaded (0.003 MB deduped)wandb: \ 0.025 MB of 0.025 MB uploaded (0.003 MB deduped)wandb:                                                                                
wandb: W&B sync reduced upload amount by 10.4%             
wandb: 
wandb: Run history:
wandb:               epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:             lr-Adam ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            test_map ▁
wandb:         test_map_50 ▁
wandb:         test_map_75 ▁
wandb:      test_map_large ▁
wandb:     test_map_medium ▁
wandb:  test_map_per_class ▁
wandb:      test_map_small ▁
wandb:           test_size ▁
wandb:    train_loss_epoch █▆▆▆▆▆▆▆▅▅▅▄▄▄▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     train_loss_step ▅▆▇█▅▆▆▆▅▆▄▄▃▃▄▃▂▃▄▃▂▃▂▂▂▁▂▂▂▁▁▁▁▁▂▁▁▂▁▂
wandb:          train_size ▁
wandb: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████
wandb: 
wandb: Run summary:
wandb:               epoch 50
wandb:             lr-Adam 0.0002
wandb:            test_map 0.09445
wandb:         test_map_50 0.22531
wandb:         test_map_75 0.05958
wandb:      test_map_large 0.11421
wandb:     test_map_medium 0.03468
wandb:  test_map_per_class -1.0
wandb:      test_map_small 0.01095
wandb:           test_size 5823
wandb:    train_loss_epoch 0.08104
wandb:     train_loss_step 0.10151
wandb:          train_size 1143
wandb: trainer/global_step 2400
wandb: 
wandb: 🚀 View run pascal_BatchBALD_cold-start_0717_chunk-1 at: https://wandb.ai/paibl/active-learning/runs/qq1n0h9q
wandb: ⭐️ View project at: https://wandb.ai/paibl/active-learning
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/BatchBALD/pascal_BatchBALD_cold-start_0717/wandb/run-20240717_134220-qq1n0h9q/logs
Predicting:   0%|          | 0/10 [00:00<?, ?it/s]Predicting:  10%|█         | 1/10 [00:03<00:28,  3.11s/it]Predicting:  20%|██        | 2/10 [00:06<00:24,  3.01s/it]Predicting:  30%|███       | 3/10 [00:08<00:20,  2.89s/it]Predicting:  40%|████      | 4/10 [00:11<00:17,  2.88s/it]Predicting:  50%|█████     | 5/10 [00:14<00:14,  2.92s/it]Predicting:  60%|██████    | 6/10 [00:17<00:11,  2.96s/it]Predicting:  70%|███████   | 7/10 [00:20<00:08,  2.94s/it]Predicting:  80%|████████  | 8/10 [00:23<00:05,  2.92s/it]Predicting:  90%|█████████ | 9/10 [00:26<00:02,  2.92s/it]Predicting: 100%|██████████| 10/10 [00:27<00:00,  2.44s/it]Predicting: 100%|██████████| 10/10 [00:27<00:00,  2.77s/it]
Conditional Entropy:   0%|          | 0/10 [00:00<?, ?it/s]                                                           BatchBALD:   0%|          | 0/114 [00:00<?, ?it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:314: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [1, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

ExactJointEntropy.compute_batch:  40%|████      | 4/10 [00:00<00:00, 38.95it/s]
                                                                               BatchBALD:   1%|          | 1/114 [00:00<00:21,  5.31it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [1, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [2, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:   3%|▎         | 3/114 [00:00<00:11,  9.53it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [3, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [4, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [5, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [6, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:   6%|▌         | 7/114 [00:00<00:05, 19.48it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [7, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [8, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [9, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [10, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  10%|▉         | 11/114 [00:00<00:04, 25.38it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [11, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [12, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [13, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [14, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  13%|█▎        | 15/114 [00:00<00:03, 28.51it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [15, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [16, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [17, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [18, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  17%|█▋        | 19/114 [00:00<00:03, 30.36it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [19, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [20, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [21, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [22, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  20%|██        | 23/114 [00:00<00:02, 31.49it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [23, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [24, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [25, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [26, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  24%|██▎       | 27/114 [00:01<00:02, 32.09it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [27, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [28, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [29, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [30, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  27%|██▋       | 31/114 [00:01<00:02, 32.57it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [31, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [32, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [33, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [34, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  31%|███       | 35/114 [00:01<00:02, 33.12it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [35, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [36, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [37, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [38, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  34%|███▍      | 39/114 [00:01<00:02, 33.36it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [39, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [40, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [41, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [42, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  38%|███▊      | 43/114 [00:01<00:02, 33.51it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [43, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [44, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [45, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [46, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  41%|████      | 47/114 [00:01<00:01, 33.58it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [47, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [48, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [49, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [50, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  45%|████▍     | 51/114 [00:01<00:01, 33.25it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [51, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [52, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [53, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [54, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  48%|████▊     | 55/114 [00:01<00:01, 32.53it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [55, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [56, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [57, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [58, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  52%|█████▏    | 59/114 [00:01<00:01, 32.17it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [59, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [60, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [61, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [62, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  55%|█████▌    | 63/114 [00:02<00:01, 31.77it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [63, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [64, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [65, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [66, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  59%|█████▉    | 67/114 [00:02<00:01, 31.37it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [67, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [68, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [69, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [70, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  62%|██████▏   | 71/114 [00:02<00:01, 30.84it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [71, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [72, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [73, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [74, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  66%|██████▌   | 75/114 [00:02<00:01, 30.43it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [75, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [76, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [77, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [78, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  69%|██████▉   | 79/114 [00:02<00:01, 30.26it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [79, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [80, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [81, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [82, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  73%|███████▎  | 83/114 [00:02<00:01, 30.15it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [83, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [84, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [85, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [86, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  76%|███████▋  | 87/114 [00:02<00:00, 30.01it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [87, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [88, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [89, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [90, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  80%|███████▉  | 91/114 [00:03<00:00, 29.94it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [91, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [92, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [93, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  82%|████████▏ | 94/114 [00:03<00:00, 29.75it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [94, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [95, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [96, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  85%|████████▌ | 97/114 [00:03<00:00, 29.48it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [97, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [98, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [99, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  88%|████████▊ | 100/114 [00:03<00:00, 29.29it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [100, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [101, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [102, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  90%|█████████ | 103/114 [00:03<00:00, 28.98it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [103, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [104, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [105, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  93%|█████████▎| 106/114 [00:03<00:00, 28.67it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [106, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [107, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [108, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  96%|█████████▌| 109/114 [00:03<00:00, 28.46it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [109, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [110, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [111, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  98%|█████████▊| 112/114 [00:03<00:00, 28.04it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [112, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [113, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                                                                                   /home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python pascal_detector.py --root_dir /group/jmearlesgrp/dat ...
INFO: GPU available: True (cuda), used: True
INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True
INFO: TPU available: False, using: 0 TPU cores
INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores
INFO: IPU available: False, using: 0 IPUs
INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs
INFO: HPU available: False, using: 0 HPUs
INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/BatchBALD/pascal_BatchBALD_cold-start_0717/wandb/run-20240717_150552-4j2w7usn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pascal_BatchBALD_cold-start_0717_chunk-2
wandb: ⭐️ View project at https://wandb.ai/paibl/active-learning
wandb: 🚀 View run at https://wandb.ai/paibl/active-learning/runs/4j2w7usn
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: 
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
INFO:lightning.pytorch.callbacks.model_summary:
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 9. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
INFO: `Trainer.fit` stopped: `max_epochs=50` reached.
INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=50` reached.
INFO: Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/BatchBALD/pascal_BatchBALD_cold-start_0717/active-learning/4j2w7usn/checkpoints/epoch=46-step=2491.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/BatchBALD/pascal_BatchBALD_cold-start_0717/active-learning/4j2w7usn/checkpoints/epoch=46-step=2491.ckpt
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/BatchBALD/pascal_BatchBALD_cold-start_0717/active-learning/4j2w7usn/checkpoints/epoch=46-step=2491.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/BatchBALD/pascal_BatchBALD_cold-start_0717/active-learning/4j2w7usn/checkpoints/epoch=46-step=2491.ckpt
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
wandb: - 0.007 MB of 0.007 MB uploadedwandb: \ 0.007 MB of 0.007 MB uploadedwandb: | 0.007 MB of 0.007 MB uploadedwandb: / 0.012 MB of 0.022 MB uploadedwandb: - 0.012 MB of 0.022 MB uploadedwandb: \ 0.025 MB of 0.025 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:             lr-Adam ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            test_map ▁
wandb:         test_map_50 ▁
wandb:         test_map_75 ▁
wandb:      test_map_large ▁
wandb:     test_map_medium ▁
wandb:  test_map_per_class ▁
wandb:      test_map_small ▁
wandb:           test_size ▁
wandb:    train_loss_epoch █▅▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     train_loss_step ▆▇▄▆▅▃█▄▆▆▆▇▆▄▃▆▆▅▅▃▃▄▃▂▁▂▂▄▂▂▃▃▂▃▁▁▂▂▁▂
wandb:          train_size ▁
wandb: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb: 
wandb: Run summary:
wandb:               epoch 50
wandb:             lr-Adam 0.0002
wandb:            test_map 0.09278
wandb:         test_map_50 0.20842
wandb:         test_map_75 0.06719
wandb:      test_map_large 0.1122
wandb:     test_map_medium 0.03409
wandb:  test_map_per_class -1.0
wandb:      test_map_small 0.00943
wandb:           test_size 5823
wandb:    train_loss_epoch 0.04831
wandb:     train_loss_step 0.0494
wandb:          train_size 1257
wandb: trainer/global_step 2650
wandb: 
wandb: 🚀 View run pascal_BatchBALD_cold-start_0717_chunk-2 at: https://wandb.ai/paibl/active-learning/runs/4j2w7usn
wandb: ⭐️ View project at: https://wandb.ai/paibl/active-learning
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/BatchBALD/pascal_BatchBALD_cold-start_0717/wandb/run-20240717_150552-4j2w7usn/logs
Predicting:   0%|          | 0/10 [00:00<?, ?it/s]Predicting:  10%|█         | 1/10 [00:02<00:22,  2.48s/it]Predicting:  20%|██        | 2/10 [00:04<00:19,  2.50s/it]Predicting:  30%|███       | 3/10 [00:07<00:17,  2.50s/it]Predicting:  40%|████      | 4/10 [00:10<00:15,  2.51s/it]Predicting:  50%|█████     | 5/10 [00:12<00:12,  2.51s/it]Predicting:  60%|██████    | 6/10 [00:15<00:10,  2.52s/it]Predicting:  70%|███████   | 7/10 [00:17<00:07,  2.52s/it]Predicting:  80%|████████  | 8/10 [00:20<00:05,  2.52s/it]Predicting:  90%|█████████ | 9/10 [00:22<00:02,  2.52s/it]Predicting: 100%|██████████| 10/10 [00:23<00:00,  2.12s/it]Predicting: 100%|██████████| 10/10 [00:23<00:00,  2.39s/it]
Conditional Entropy:   0%|          | 0/10 [00:00<?, ?it/s]                                                           BatchBALD:   0%|          | 0/114 [00:00<?, ?it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:314: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [1, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [3, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [6, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [9, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [4, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:   4%|▍         | 5/114 [00:00<00:02, 42.57it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [5, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]
                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [7, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [8, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]
                                                                       BatchBALD:   9%|▉         | 10/114 [00:00<00:02, 41.20it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [10, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [11, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [12, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [13, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [14, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  13%|█▎        | 15/114 [00:00<00:02, 39.78it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [15, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [16, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [17, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [18, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  17%|█▋        | 19/114 [00:00<00:02, 38.80it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [19, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [20, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [21, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [22, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  20%|██        | 23/114 [00:00<00:02, 38.36it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [23, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [24, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [25, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [26, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  24%|██▎       | 27/114 [00:00<00:02, 38.28it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [27, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [28, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [29, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [30, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  27%|██▋       | 31/114 [00:00<00:02, 37.93it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [31, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [32, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [33, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [34, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  31%|███       | 35/114 [00:00<00:02, 37.25it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [35, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [36, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [37, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [38, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  34%|███▍      | 39/114 [00:01<00:02, 36.80it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [39, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [40, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [41, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [42, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  38%|███▊      | 43/114 [00:01<00:01, 36.26it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [43, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [44, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [45, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [46, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  41%|████      | 47/114 [00:01<00:01, 35.92it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [47, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [48, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [49, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [50, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  45%|████▍     | 51/114 [00:01<00:01, 35.68it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [51, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [52, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [53, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [54, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  48%|████▊     | 55/114 [00:01<00:01, 35.70it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [55, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [56, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [57, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [58, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  52%|█████▏    | 59/114 [00:01<00:01, 35.57it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [59, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [60, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [61, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [62, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  55%|█████▌    | 63/114 [00:01<00:01, 35.42it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [63, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [64, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [65, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [66, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  59%|█████▉    | 67/114 [00:01<00:01, 35.04it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [67, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [68, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [69, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [70, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  62%|██████▏   | 71/114 [00:01<00:01, 34.48it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [71, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [72, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [73, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [74, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  66%|██████▌   | 75/114 [00:02<00:01, 33.82it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [75, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [76, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [77, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [78, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  69%|██████▉   | 79/114 [00:02<00:01, 33.26it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [79, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [80, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [81, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [82, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  73%|███████▎  | 83/114 [00:02<00:00, 32.70it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [83, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [84, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [85, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [86, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  76%|███████▋  | 87/114 [00:02<00:00, 32.41it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [87, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [88, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [89, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [90, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  80%|███████▉  | 91/114 [00:02<00:00, 32.07it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [91, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [92, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [93, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [94, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  83%|████████▎ | 95/114 [00:02<00:00, 31.83it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [95, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [96, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [97, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [98, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  87%|████████▋ | 99/114 [00:02<00:00, 31.35it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [99, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [100, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [101, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [102, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  90%|█████████ | 103/114 [00:02<00:00, 31.38it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [103, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [104, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [105, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [106, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  94%|█████████▍| 107/114 [00:03<00:00, 31.21it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [107, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [108, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [109, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [110, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  97%|█████████▋| 111/114 [00:03<00:00, 30.80it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [111, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [112, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [113, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                                                                                   /home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python pascal_detector.py --root_dir /group/jmearlesgrp/dat ...
INFO: GPU available: True (cuda), used: True
INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True
INFO: TPU available: False, using: 0 TPU cores
INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores
INFO: IPU available: False, using: 0 IPUs
INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs
INFO: HPU available: False, using: 0 HPUs
INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/BatchBALD/pascal_BatchBALD_cold-start_0717/wandb/run-20240717_163404-9dg3n0vn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pascal_BatchBALD_cold-start_0717_chunk-3
wandb: ⭐️ View project at https://wandb.ai/paibl/active-learning
wandb: 🚀 View run at https://wandb.ai/paibl/active-learning/runs/9dg3n0vn
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: 
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
INFO:lightning.pytorch.callbacks.model_summary:
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 3. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
INFO: `Trainer.fit` stopped: `max_epochs=50` reached.
INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=50` reached.
INFO: Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/BatchBALD/pascal_BatchBALD_cold-start_0717/active-learning/9dg3n0vn/checkpoints/epoch=45-step=2668.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/BatchBALD/pascal_BatchBALD_cold-start_0717/active-learning/9dg3n0vn/checkpoints/epoch=45-step=2668.ckpt
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/BatchBALD/pascal_BatchBALD_cold-start_0717/active-learning/9dg3n0vn/checkpoints/epoch=45-step=2668.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/BatchBALD/pascal_BatchBALD_cold-start_0717/active-learning/9dg3n0vn/checkpoints/epoch=45-step=2668.ckpt
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
wandb: - 0.007 MB of 0.007 MB uploadedwandb: \ 0.007 MB of 0.019 MB uploadedwandb: | 0.019 MB of 0.019 MB uploadedwandb: / 0.019 MB of 0.019 MB uploadedwandb: - 0.019 MB of 0.019 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:             lr-Adam ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            test_map ▁
wandb:         test_map_50 ▁
wandb:         test_map_75 ▁
wandb:      test_map_large ▁
wandb:     test_map_medium ▁
wandb:  test_map_per_class ▁
wandb:      test_map_small ▁
wandb:           test_size ▁
wandb:    train_loss_epoch █▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▅▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁
wandb:     train_loss_step █▄▄▃▄▃▃▅▄▄▃▅▂▃▄▃▂▃▂▃▃▂▂▂▇▃▃▃▃▂▃▂▂▄▃▂▂▁▂▄
wandb:          train_size ▁
wandb: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb: 
wandb: Run summary:
wandb:               epoch 50
wandb:             lr-Adam 0.0002
wandb:            test_map 0.09134
wandb:         test_map_50 0.20195
wandb:         test_map_75 0.06859
wandb:      test_map_large 0.11088
wandb:     test_map_medium 0.03523
wandb:  test_map_per_class -1.0
wandb:      test_map_small 0.01371
wandb:           test_size 5823
wandb:    train_loss_epoch 0.03727
wandb:     train_loss_step 0.05019
wandb:          train_size 1371
wandb: trainer/global_step 2900
wandb: 
wandb: 🚀 View run pascal_BatchBALD_cold-start_0717_chunk-3 at: https://wandb.ai/paibl/active-learning/runs/9dg3n0vn
wandb: ⭐️ View project at: https://wandb.ai/paibl/active-learning
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/BatchBALD/pascal_BatchBALD_cold-start_0717/wandb/run-20240717_163404-9dg3n0vn/logs
Predicting:   0%|          | 0/10 [00:00<?, ?it/s]Predicting:  10%|█         | 1/10 [00:02<00:22,  2.50s/it]Predicting:  20%|██        | 2/10 [00:05<00:20,  2.51s/it]Predicting:  30%|███       | 3/10 [00:07<00:17,  2.51s/it]Predicting:  40%|████      | 4/10 [00:10<00:15,  2.52s/it]Predicting:  50%|█████     | 5/10 [00:12<00:12,  2.52s/it]Predicting:  60%|██████    | 6/10 [00:15<00:10,  2.52s/it]Predicting:  70%|███████   | 7/10 [00:17<00:07,  2.52s/it]Predicting:  80%|████████  | 8/10 [00:20<00:05,  2.52s/it]Predicting:  90%|█████████ | 9/10 [00:22<00:02,  2.52s/it]Predicting: 100%|██████████| 10/10 [00:23<00:00,  2.13s/it]Predicting: 100%|██████████| 10/10 [00:23<00:00,  2.39s/it]
Conditional Entropy:   0%|          | 0/10 [00:00<?, ?it/s]                                                           BatchBALD:   0%|          | 0/114 [00:00<?, ?it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:314: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [1, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [7, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [2, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [3, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [4, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:   4%|▍         | 5/114 [00:00<00:02, 42.68it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [5, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [6, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]
                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [8, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [9, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:   9%|▉         | 10/114 [00:00<00:02, 40.96it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [10, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [11, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [12, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [13, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [14, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  13%|█▎        | 15/114 [00:00<00:02, 39.31it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [15, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [16, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [17, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [18, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  17%|█▋        | 19/114 [00:00<00:02, 39.28it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [19, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [20, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [21, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [22, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [23, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  21%|██        | 24/114 [00:00<00:02, 40.09it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [24, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [25, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [26, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [27, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [28, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  25%|██▌       | 29/114 [00:00<00:02, 40.79it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [29, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [30, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [31, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [32, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [33, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  30%|██▉       | 34/114 [00:00<00:01, 41.55it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [34, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [35, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [36, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [37, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [38, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  34%|███▍      | 39/114 [00:00<00:01, 41.39it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [39, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [40, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [41, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [42, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [43, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  39%|███▊      | 44/114 [00:01<00:01, 41.47it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [44, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [45, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [46, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [47, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [48, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  43%|████▎     | 49/114 [00:01<00:01, 40.44it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [49, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [50, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [51, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [52, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [53, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  47%|████▋     | 54/114 [00:01<00:01, 39.33it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [54, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [55, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [56, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [57, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  51%|█████     | 58/114 [00:01<00:01, 39.49it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [58, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [59, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [60, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [61, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  54%|█████▍    | 62/114 [00:01<00:01, 38.67it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [62, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [63, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [64, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [65, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  58%|█████▊    | 66/114 [00:01<00:01, 38.26it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [66, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [67, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [68, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [69, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  61%|██████▏   | 70/114 [00:01<00:01, 38.50it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [70, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [71, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [72, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [73, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  65%|██████▍   | 74/114 [00:01<00:01, 38.68it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [74, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [75, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [76, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [77, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  68%|██████▊   | 78/114 [00:01<00:00, 38.73it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [78, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [79, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [80, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [81, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  72%|███████▏  | 82/114 [00:02<00:00, 38.64it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [82, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [83, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [84, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [85, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  75%|███████▌  | 86/114 [00:02<00:00, 38.49it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [86, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [87, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [88, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [89, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  79%|███████▉  | 90/114 [00:02<00:00, 38.06it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [90, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [91, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [92, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [93, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  82%|████████▏ | 94/114 [00:02<00:00, 37.78it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [94, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [95, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [96, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [97, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  86%|████████▌ | 98/114 [00:02<00:00, 36.53it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [98, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [99, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [100, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [101, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  89%|████████▉ | 102/114 [00:02<00:00, 35.64it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [102, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [103, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [104, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [105, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  93%|█████████▎| 106/114 [00:02<00:00, 35.81it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [106, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [107, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [108, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [109, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD:  96%|█████████▋| 110/114 [00:02<00:00, 35.82it/s]
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [110, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [111, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [112, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       
ExactJointEntropy.compute_batch:   0%|          | 0/10 [00:00<?, ?it/s]/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py:506: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [113, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:24.)
  torch.matmul(

                                                                       BatchBALD: 100%|██████████| 114/114 [00:02<00:00, 35.78it/s]                                                            /home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python pascal_detector.py --root_dir /group/jmearlesgrp/dat ...
INFO: GPU available: True (cuda), used: True
INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True
INFO: TPU available: False, using: 0 TPU cores
INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores
INFO: IPU available: False, using: 0 IPUs
INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs
INFO: HPU available: False, using: 0 HPUs
INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/BatchBALD/pascal_BatchBALD_cold-start_0717/wandb/run-20240717_180903-1z6hs1bf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pascal_BatchBALD_cold-start_0717_chunk-4
wandb: ⭐️ View project at https://wandb.ai/paibl/active-learning
wandb: 🚀 View run at https://wandb.ai/paibl/active-learning/runs/1z6hs1bf
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: 
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
INFO:lightning.pytorch.callbacks.model_summary:
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 21. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
slurmstepd: error: *** JOB 14572465 ON gpu-5-50 CANCELLED AT 2024-07-17T18:40:28 ***
