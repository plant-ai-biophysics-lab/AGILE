Unloading openmpi/4.1.5
Unloading slurm/23.02.7
Loading slurm/23.02.7
Loading openmpi/4.1.5
INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.11 (you have 1.4.8). Upgrade using: pip install --upgrade albumentations
INFO: [rank: 0] Seed set to 42
INFO:lightning.fabric.utilities.seed:[rank: 0] Seed set to 42
INFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /tmp/tmpn9twv523
INFO:torch.distributed.nn.jit.instantiator:Writing /tmp/tmpn9twv523/_remote_module_non_scriptable.py
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python pascal_detector.py --root_dir /group/jmearlesgrp/dat ...
INFO: GPU available: True (cuda), used: True
INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True
INFO: TPU available: False, using: 0 TPU cores
INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores
INFO: IPU available: False, using: 0 IPUs
INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs
INFO: HPU available: False, using: 0 HPUs
INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs
wandb: Currently logged in as: earlranario (paibl). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/entropy/pascal_entropy_no-ckpt_0717/wandb/run-20240718_050546-amhsrr2h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pascal_entropy_no-ckpt_0717_chunk-1
wandb: â­ï¸ View project at https://wandb.ai/paibl/active-learning
wandb: ğŸš€ View run at https://wandb.ai/paibl/active-learning/runs/amhsrr2h
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: 
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
INFO:lightning.pytorch.callbacks.model_summary:
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (24) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 24. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 19. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
INFO: `Trainer.fit` stopped: `max_epochs=100` reached.
INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.
INFO: Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/entropy/pascal_entropy_no-ckpt_0717/active-learning/amhsrr2h/checkpoints/epoch=96-step=2328.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/entropy/pascal_entropy_no-ckpt_0717/active-learning/amhsrr2h/checkpoints/epoch=96-step=2328.ckpt
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/entropy/pascal_entropy_no-ckpt_0717/active-learning/amhsrr2h/checkpoints/epoch=96-step=2328.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/entropy/pascal_entropy_no-ckpt_0717/active-learning/amhsrr2h/checkpoints/epoch=96-step=2328.ckpt
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
wandb: - 0.007 MB of 0.007 MB uploadedwandb: \ 0.007 MB of 0.019 MB uploadedwandb: | 0.007 MB of 0.020 MB uploadedwandb: / 0.007 MB of 0.020 MB uploadedwandb: - 0.020 MB of 0.020 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:             lr-Adam â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            test_map â–
wandb:         test_map_50 â–
wandb:         test_map_75 â–
wandb:      test_map_large â–
wandb:     test_map_medium â–
wandb:  test_map_per_class â–
wandb:      test_map_small â–
wandb:           test_size â–
wandb:    train_loss_epoch â–ˆâ–…â–…â–…â–…â–…â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     train_loss_step â–†â–ˆâ–†â–ˆâ–‡â–†â–…â–ƒâ–„â–…â–ƒâ–‚â–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–
wandb:          train_size â–
wandb: trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 100
wandb:             lr-Adam 0.0002
wandb:            test_map 0.05332
wandb:         test_map_50 0.14112
wandb:         test_map_75 0.02921
wandb:      test_map_large 0.06507
wandb:     test_map_medium 0.01587
wandb:  test_map_per_class -1.0
wandb:      test_map_small 0.0046
wandb:           test_size 5823
wandb:    train_loss_epoch 0.05489
wandb:     train_loss_step 0.05829
wandb:          train_size 571
wandb: trainer/global_step 2400
wandb: 
wandb: ğŸš€ View run pascal_entropy_no-ckpt_0717_chunk-1 at: https://wandb.ai/paibl/active-learning/runs/amhsrr2h
wandb: â­ï¸ View project at: https://wandb.ai/paibl/active-learning
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/entropy/pascal_entropy_no-ckpt_0717/wandb/run-20240718_050546-amhsrr2h/logs
Traceback (most recent call last):
  File "/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/pascal_detector.py", line 107, in <module>
    main(
  File "/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/pascal_detector.py", line 41, in main
    train_dataset, test_dataset = us.sample(chunk=chunk, method=method, model=model, dm=PASCALDataModule, batch_size=batch_size)
  File "/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/sampling.py", line 55, in sample
    return self.entropy_based(chunk, model, dm, batch_size)
  File "/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/sampling.py", line 142, in entropy_based
    _, probs = model.predict_step(inputs, dropout=False)
  File "/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/models.py", line 182, in predict_step
    _, logits = self.model(x) # logits is a list of tensors of varying sizes
  File "/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/group/jmearlesgrp/scratch/eranario/Active-Deep-Learning/scripts/active-learning/src/util.py", line 51, in forward
    outputs = super(CustomFasterRCNN, self).forward(images, targets)
  File "/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/torchvision/models/detection/generalized_rcnn.py", line 101, in forward
    features = self.backbone(images.tensors)
  File "/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/torchvision/models/detection/backbone_utils.py", line 57, in forward
    x = self.body(x)
  File "/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/torchvision/models/_utils.py", line 69, in forward
    x = module(x)
  File "/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 457, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 453, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same

-------------------------------------------------------------------------------
pascal_detector.py 107 <module>
main(

pascal_detector.py 41 main
train_dataset, test_dataset = us.sample(chunk=chunk, method=method, model=model, dm=PASCALDataModule, batch_size=batch_size)

sampling.py 55 sample
return self.entropy_based(chunk, model, dm, batch_size)

sampling.py 142 entropy_based
_, probs = model.predict_step(inputs, dropout=False)

models.py 182 predict_step
_, logits = self.model(x) # logits is a list of tensors of varying sizes

module.py 1130 _call_impl
return forward_call(*input, **kwargs)

util.py 51 forward
outputs = super(CustomFasterRCNN, self).forward(images, targets)

generalized_rcnn.py 101 forward
features = self.backbone(images.tensors)

module.py 1130 _call_impl
return forward_call(*input, **kwargs)

backbone_utils.py 57 forward
x = self.body(x)

module.py 1130 _call_impl
return forward_call(*input, **kwargs)

_utils.py 69 forward
x = module(x)

module.py 1130 _call_impl
return forward_call(*input, **kwargs)

conv.py 457 forward
return self._conv_forward(input, self.weight, self.bias)

conv.py 453 _conv_forward
return F.conv2d(input, weight, bias, self.stride,

RuntimeError:
Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same
