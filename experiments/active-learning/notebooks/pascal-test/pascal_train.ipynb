{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import albumentations as A\n",
    "import torchmetrics\n",
    "\n",
    "root_dir = '/data2/eranario/data/PASCAL-VOC-2012'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dstaset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PASCALDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, root_dir, split='train', year='2012', transform=None):\n",
    "        super().__init__()\n",
    "        self.split = split\n",
    "        self.year = year\n",
    "        self.transform = transform\n",
    "        self.num_classes = 20\n",
    "        \n",
    "        # directory of images and labels\n",
    "        self.root_dir = root_dir\n",
    "        self.img_dir = os.path.join(root_dir, 'VOCdevkit', 'VOC' + year, 'JPEGImages')\n",
    "        self.labels_dir = os.path.join(root_dir, 'VOCdevkit', 'VOC' + year, 'Annotations')\n",
    "        \n",
    "        # splits directory\n",
    "        self.splits_dir = os.path.join(self.root_dir, 'VOCdevkit', 'VOC' + year, 'ImageSets', 'Main')\n",
    "        \n",
    "        # load splits\n",
    "        self.img_ids = []\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.load_data()\n",
    "        print('Loaded {} images and {} labels'.format(len(self.images), len(self.labels)))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # load image\n",
    "        image = cv2.imread(self.images[idx], cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # load xml label as dictionary\n",
    "        label_path = self.labels[idx]\n",
    "        boxes, labels = self.parse_voc_xml(label_path)\n",
    "        \n",
    "        # add transform\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, bboxes=boxes, labels=labels)\n",
    "            image = augmented['image']\n",
    "            boxes = torch.tensor(augmented['bboxes'], dtype=torch.float32)\n",
    "            labels = torch.tensor(augmented['labels'], dtype=torch.long)\n",
    "        \n",
    "        return image, {'boxes': boxes, 'labels': labels}\n",
    "    \n",
    "    def load_data(self):\n",
    "        \n",
    "        with open(os.path.join(self.splits_dir, self.split + '.txt')) as f:\n",
    "            for line in f:\n",
    "                self.img_ids.append(line.strip())\n",
    "                \n",
    "        for img_id in self.img_ids:\n",
    "            img_file = os.path.join(self.img_dir, img_id + '.jpg')\n",
    "            ann_file = os.path.join(self.labels_dir, img_id + '.xml')\n",
    "            self.images.append(img_file)\n",
    "            self.labels.append(ann_file)\n",
    "            \n",
    "        assert len(self.images) == len(self.labels)\n",
    "        \n",
    "    def parse_voc_xml(self, annotation_path):\n",
    "        import xml.etree.ElementTree as ET\n",
    "        tree = ET.parse(annotation_path)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for obj in root.findall('object'):\n",
    "            label = obj.find('name').text\n",
    "            bbox = obj.find('bndbox')\n",
    "            box = [\n",
    "                int(bbox.find('xmin').text),\n",
    "                int(bbox.find('ymin').text),\n",
    "                int(bbox.find('xmax').text),\n",
    "                int(bbox.find('ymax').text)\n",
    "            ]\n",
    "            boxes.append(box)\n",
    "            labels.append(label)  # Labels are still strings here\n",
    "\n",
    "        # Convert labels to integers\n",
    "        label_map = {'aeroplane': 0, 'bicycle': 1, 'bird': 2, 'boat': 3, 'bottle': 4,\n",
    "                     'bus': 5, 'car': 6, 'cat': 7, 'chair': 8, 'cow': 9, 'diningtable': 10,\n",
    "                     'dog': 11, 'horse': 12, 'motorbike': 13, 'person': 14, 'pottedplant': 15,\n",
    "                     'sheep': 16, 'sofa': 17, 'train': 18, 'tvmonitor': 19}\n",
    "        labels = [label_map[label] for label in labels]\n",
    "        \n",
    "        return boxes, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "from torchvision.datasets import VOCDetection\n",
    "import lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "class PASCALDataModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, root_dir: str, batch_size: int = 32, num_workers: int = 4, image_size = 512):\n",
    "        \"\"\"Data Module for handling PASCAL VOC 2012 dataset.\n",
    "\n",
    "        Args:\n",
    "            root_dir (str): Root directory of the PASCAL VOC dataset.\n",
    "            batch_size (int, optional): Number of samples per batch. Defaults to 32.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.root_dir = root_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        self.train_transform = A.Compose([\n",
    "            A.Resize(self.image_size, self.image_size),\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2()\n",
    "        ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
    "        \n",
    "        self.test_transform = A.Compose([\n",
    "            A.Resize(self.image_size, self.image_size),\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2()\n",
    "        ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # Download the Pascal VOC dataset\n",
    "        VOCDetection(self.root_dir, year='2012', image_set='train', download=False)\n",
    "        VOCDetection(self.root_dir, year='2012', image_set='val', download=False)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == 'fit' or stage is None:\n",
    "            print(\"Setting up training datasets\")\n",
    "            full_train_dataset = PASCALDataset(self.root_dir, split='train', transform=self.train_transform)\n",
    "            \n",
    "            train_size = int(0.8 * len(full_train_dataset))\n",
    "            val_size = len(full_train_dataset) - train_size\n",
    "            \n",
    "            self.train_dataset, self.val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "        \n",
    "        if stage == 'test' or stage is None:\n",
    "            self.test_dataset = PASCALDataset(self.root_dir, split='val', transform=self.test_transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers, collate_fn=self.collate_fn)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers, collate_fn=self.collate_fn)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers, collate_fn=self.collate_fn)\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        images, targets = zip(*batch)\n",
    "        images = torch.stack(images)\n",
    "        boxes = [target['boxes'] for target in targets]\n",
    "        labels = [target['labels'] for target in targets]\n",
    "        return images, {'boxes': boxes, 'labels': labels}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "\n",
    "class LitDetectorModel(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, num_classes: int = 1, learning_rate: float = 2e-4):\n",
    "        \"\"\"Object Detection model built with PyTorch Lightning using Faster R-CNN.\n",
    "\n",
    "        Args:\n",
    "            num_classes (int, optional): Number of classes. Defaults to 1.\n",
    "            learning_rate (float, optional): Rate at which to adjust model weights. Defaults to 2e-4.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define properties\n",
    "        self.save_hyperparameters()\n",
    "        self.hparams.lr = learning_rate\n",
    "        self.hparams.num_classes = num_classes\n",
    "        \n",
    "        # Define the model\n",
    "        self.model = fasterrcnn_resnet50_fpn(pretrained=False, num_classes=num_classes)\n",
    "        \n",
    "        # mAP calculation\n",
    "        self.val_map_metric = torchmetrics.detection.MeanAveragePrecision(box_format='xyxy')\n",
    "        self.test_map_metric = torchmetrics.detection.MeanAveragePrecision(box_format='xyxy')\n",
    "        self.validation_outputs = []\n",
    "        self.test_outputs = []\n",
    "\n",
    "    def forward(self, images, targets=None):\n",
    "        return self.model(images, targets)\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, targets = batch\n",
    "        targets = self.format_targets(targets)\n",
    "        loss_dict = self.model(images, targets)\n",
    "        \n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        self.log('train_loss', losses, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return losses\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, targets = batch\n",
    "        targets = self.format_targets(targets)\n",
    "        outputs = self.model(images)\n",
    "        \n",
    "        preds = [{k: v.detach() for k, v in t.items()} for t in outputs]\n",
    "        \n",
    "        # Wrap targets in a list of dictionaries\n",
    "        formatted_targets = targets\n",
    "        \n",
    "        self.val_map_metric.update(preds, formatted_targets)\n",
    "        self.validation_outputs.append({'preds': preds, 'targets': formatted_targets})\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        if not self.validation_outputs:\n",
    "            mAP_result = {'map': torch.tensor(0.0)}\n",
    "        else:\n",
    "            mAP_result = self.val_map_metric.compute()\n",
    "            self.val_map_metric.reset()\n",
    "\n",
    "        # Log only the keys that contain \"map\"\n",
    "        map_keys = {key: value for key, value in mAP_result.items() if 'map' in key}\n",
    "        for key, value in map_keys.items():\n",
    "            self.log(f'val_{key}', value, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "        self.validation_outputs.clear()\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        images, targets = batch\n",
    "        targets = self.format_targets(targets)\n",
    "        outputs = self.model(images)\n",
    "        \n",
    "        preds = [{k: v.detach() for k, v in t.items()} for t in outputs]\n",
    "        \n",
    "        # Wrap targets in a list of dictionaries\n",
    "        formatted_targets = targets\n",
    "        \n",
    "        self.test_map_metric.update(preds, formatted_targets)\n",
    "        self.test_outputs.append({'preds': preds, 'targets': formatted_targets})\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def on_test_epoch_end(self):\n",
    "        if not self.test_outputs:\n",
    "            mAP_result = {'map': torch.tensor(0.0)}\n",
    "        else:\n",
    "            mAP_result = self.test_map_metric.compute()\n",
    "            self.test_map_metric.reset()\n",
    "\n",
    "        # Log only the keys that contain \"map\"\n",
    "        map_keys = {key: value for key, value in mAP_result.items() if 'map' in key}\n",
    "        for key, value in map_keys.items():\n",
    "            self.log(f'test_{key}', value, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "        self.test_outputs.clear()\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "    \n",
    "    def format_targets(self, targets):\n",
    "        \"\"\"Convert the targets to the format expected by the model.\"\"\"\n",
    "        formatted_targets = []\n",
    "        for boxes, labels in zip(targets['boxes'], targets['labels']):\n",
    "            formatted_targets.append({\n",
    "                'boxes': boxes,\n",
    "                'labels': labels\n",
    "            })\n",
    "        return formatted_targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up training datasets\n",
      "Loaded 5717 images and 5717 labels\n",
      "Loaded 5823 images and 5823 labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /tmp/tmpu4noxy9_\n",
      "INFO:torch.distributed.nn.jit.instantiator:Writing /tmp/tmpu4noxy9_/_remote_module_non_scriptable.py\n",
      "/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/torchvision/models/_utils.py:208: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/torchvision/models/_utils.py:223: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: earlranario (paibl). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/eranario/intermediate_data/Active-Learning/PASCAL_logs/tests/wandb/run-20240613_181335-pdutn14m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/paibl/active-learning/runs/pdutn14m' target=\"_blank\">test</a></strong> to <a href='https://wandb.ai/paibl/active-learning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/paibl/active-learning' target=\"_blank\">https://wandb.ai/paibl/active-learning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/paibl/active-learning/runs/pdutn14m' target=\"_blank\">https://wandb.ai/paibl/active-learning/runs/pdutn14m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name            | Type                 | Params\n",
      "---------------------------------------------------------\n",
      "0 | model           | FasterRCNN           | 41.4 M\n",
      "1 | val_map_metric  | MeanAveragePrecision | 0     \n",
      "2 | test_map_metric | MeanAveragePrecision | 0     \n",
      "---------------------------------------------------------\n",
      "41.2 M    Trainable params\n",
      "222 K     Non-trainable params\n",
      "41.4 M    Total params\n",
      "165.566   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/382 [00:00<?, ?it/s]                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 12. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|█████████▉| 381/382 [05:28<00:00,  1.16it/s, v_num=n14m, train_loss_step=0.538]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 382/382 [05:28<00:00,  1.16it/s, v_num=n14m, train_loss_step=1.490]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\n",
      "Validation:   0%|          | 0/96 [00:00<?, ?it/s]\n",
      "Validation DataLoader 0:   0%|          | 0/96 [00:00<?, ?it/s]\n",
      "Validation DataLoader 0:   1%|          | 1/96 [00:00<00:36,  2.60it/s]\n",
      "Validation DataLoader 0:   2%|▏         | 2/96 [00:00<00:36,  2.56it/s]\n",
      "Validation DataLoader 0:   3%|▎         | 3/96 [00:01<00:36,  2.55it/s]\n",
      "Validation DataLoader 0:   4%|▍         | 4/96 [00:01<00:36,  2.53it/s]\n",
      "Validation DataLoader 0:   5%|▌         | 5/96 [00:01<00:36,  2.51it/s]\n",
      "Validation DataLoader 0:   6%|▋         | 6/96 [00:02<00:35,  2.51it/s]\n",
      "Validation DataLoader 0:   7%|▋         | 7/96 [00:02<00:35,  2.51it/s]\n",
      "Validation DataLoader 0:   8%|▊         | 8/96 [00:03<00:35,  2.51it/s]\n",
      "Validation DataLoader 0:   9%|▉         | 9/96 [00:03<00:34,  2.50it/s]\n",
      "Validation DataLoader 0:  10%|█         | 10/96 [00:03<00:34,  2.50it/s]\n",
      "Validation DataLoader 0:  11%|█▏        | 11/96 [00:04<00:34,  2.50it/s]\n",
      "Validation DataLoader 0:  12%|█▎        | 12/96 [00:04<00:33,  2.50it/s]\n",
      "Validation DataLoader 0:  14%|█▎        | 13/96 [00:05<00:33,  2.50it/s]\n",
      "Validation DataLoader 0:  15%|█▍        | 14/96 [00:05<00:32,  2.50it/s]\n",
      "Validation DataLoader 0:  16%|█▌        | 15/96 [00:06<00:32,  2.50it/s]\n",
      "Validation DataLoader 0:  17%|█▋        | 16/96 [00:06<00:32,  2.50it/s]\n",
      "Validation DataLoader 0:  18%|█▊        | 17/96 [00:06<00:31,  2.49it/s]\n",
      "Validation DataLoader 0:  19%|█▉        | 18/96 [00:07<00:31,  2.49it/s]\n",
      "Validation DataLoader 0:  20%|█▉        | 19/96 [00:07<00:30,  2.49it/s]\n",
      "Validation DataLoader 0:  21%|██        | 20/96 [00:08<00:30,  2.49it/s]\n",
      "Validation DataLoader 0:  22%|██▏       | 21/96 [00:08<00:30,  2.49it/s]\n",
      "Validation DataLoader 0:  23%|██▎       | 22/96 [00:08<00:29,  2.49it/s]\n",
      "Validation DataLoader 0:  24%|██▍       | 23/96 [00:09<00:29,  2.49it/s]\n",
      "Validation DataLoader 0:  25%|██▌       | 24/96 [00:09<00:28,  2.49it/s]\n",
      "Validation DataLoader 0:  26%|██▌       | 25/96 [00:10<00:28,  2.49it/s]\n",
      "Validation DataLoader 0:  27%|██▋       | 26/96 [00:10<00:28,  2.49it/s]\n",
      "Validation DataLoader 0:  28%|██▊       | 27/96 [00:10<00:27,  2.49it/s]\n",
      "Validation DataLoader 0:  29%|██▉       | 28/96 [00:11<00:27,  2.49it/s]\n",
      "Validation DataLoader 0:  30%|███       | 29/96 [00:11<00:26,  2.49it/s]\n",
      "Validation DataLoader 0:  31%|███▏      | 30/96 [00:12<00:26,  2.49it/s]\n",
      "Validation DataLoader 0:  32%|███▏      | 31/96 [00:12<00:26,  2.49it/s]\n",
      "Validation DataLoader 0:  33%|███▎      | 32/96 [00:12<00:25,  2.49it/s]\n",
      "Validation DataLoader 0:  34%|███▍      | 33/96 [00:13<00:25,  2.49it/s]\n",
      "Validation DataLoader 0:  35%|███▌      | 34/96 [00:13<00:24,  2.48it/s]\n",
      "Validation DataLoader 0:  36%|███▋      | 35/96 [00:14<00:24,  2.48it/s]\n",
      "Validation DataLoader 0:  38%|███▊      | 36/96 [00:14<00:24,  2.48it/s]\n",
      "Validation DataLoader 0:  39%|███▊      | 37/96 [00:14<00:23,  2.48it/s]\n",
      "Validation DataLoader 0:  40%|███▉      | 38/96 [00:15<00:23,  2.48it/s]\n",
      "Validation DataLoader 0:  41%|████      | 39/96 [00:15<00:22,  2.48it/s]\n",
      "Validation DataLoader 0:  42%|████▏     | 40/96 [00:16<00:22,  2.48it/s]\n",
      "Validation DataLoader 0:  43%|████▎     | 41/96 [00:16<00:22,  2.48it/s]\n",
      "Validation DataLoader 0:  44%|████▍     | 42/96 [00:16<00:21,  2.48it/s]\n",
      "Validation DataLoader 0:  45%|████▍     | 43/96 [00:17<00:21,  2.48it/s]\n",
      "Validation DataLoader 0:  46%|████▌     | 44/96 [00:17<00:20,  2.48it/s]\n",
      "Validation DataLoader 0:  47%|████▋     | 45/96 [00:18<00:20,  2.48it/s]\n",
      "Validation DataLoader 0:  48%|████▊     | 46/96 [00:18<00:20,  2.48it/s]\n",
      "Validation DataLoader 0:  49%|████▉     | 47/96 [00:18<00:19,  2.48it/s]\n",
      "Validation DataLoader 0:  50%|█████     | 48/96 [00:19<00:19,  2.48it/s]\n",
      "Validation DataLoader 0:  51%|█████     | 49/96 [00:19<00:18,  2.48it/s]\n",
      "Validation DataLoader 0:  52%|█████▏    | 50/96 [00:20<00:18,  2.48it/s]\n",
      "Validation DataLoader 0:  53%|█████▎    | 51/96 [00:20<00:18,  2.48it/s]\n",
      "Validation DataLoader 0:  54%|█████▍    | 52/96 [00:20<00:17,  2.48it/s]\n",
      "Validation DataLoader 0:  55%|█████▌    | 53/96 [00:21<00:17,  2.48it/s]\n",
      "Validation DataLoader 0:  56%|█████▋    | 54/96 [00:21<00:16,  2.48it/s]\n",
      "Validation DataLoader 0:  57%|█████▋    | 55/96 [00:22<00:16,  2.48it/s]\n",
      "Validation DataLoader 0:  58%|█████▊    | 56/96 [00:22<00:16,  2.48it/s]\n",
      "Validation DataLoader 0:  59%|█████▉    | 57/96 [00:22<00:15,  2.48it/s]\n",
      "Validation DataLoader 0:  60%|██████    | 58/96 [00:23<00:15,  2.48it/s]\n",
      "Validation DataLoader 0:  61%|██████▏   | 59/96 [00:23<00:14,  2.48it/s]\n",
      "Validation DataLoader 0:  62%|██████▎   | 60/96 [00:24<00:14,  2.48it/s]\n",
      "Validation DataLoader 0:  64%|██████▎   | 61/96 [00:24<00:14,  2.48it/s]\n",
      "Validation DataLoader 0:  65%|██████▍   | 62/96 [00:24<00:13,  2.48it/s]\n",
      "Validation DataLoader 0:  66%|██████▌   | 63/96 [00:25<00:13,  2.48it/s]\n",
      "Validation DataLoader 0:  67%|██████▋   | 64/96 [00:25<00:12,  2.48it/s]\n",
      "Validation DataLoader 0:  68%|██████▊   | 65/96 [00:26<00:12,  2.48it/s]\n",
      "Validation DataLoader 0:  69%|██████▉   | 66/96 [00:26<00:12,  2.48it/s]\n",
      "Validation DataLoader 0:  70%|██████▉   | 67/96 [00:27<00:11,  2.48it/s]\n",
      "Validation DataLoader 0:  71%|███████   | 68/96 [00:27<00:11,  2.48it/s]\n",
      "Validation DataLoader 0:  72%|███████▏  | 69/96 [00:27<00:10,  2.48it/s]\n",
      "Validation DataLoader 0:  73%|███████▎  | 70/96 [00:28<00:10,  2.48it/s]\n",
      "Validation DataLoader 0:  74%|███████▍  | 71/96 [00:28<00:10,  2.48it/s]\n",
      "Validation DataLoader 0:  75%|███████▌  | 72/96 [00:29<00:09,  2.48it/s]\n",
      "Validation DataLoader 0:  76%|███████▌  | 73/96 [00:29<00:09,  2.48it/s]\n",
      "Validation DataLoader 0:  77%|███████▋  | 74/96 [00:29<00:08,  2.48it/s]\n",
      "Validation DataLoader 0:  78%|███████▊  | 75/96 [00:30<00:08,  2.48it/s]\n",
      "Validation DataLoader 0:  79%|███████▉  | 76/96 [00:30<00:08,  2.48it/s]\n",
      "Validation DataLoader 0:  80%|████████  | 77/96 [00:31<00:07,  2.48it/s]\n",
      "Validation DataLoader 0:  81%|████████▏ | 78/96 [00:31<00:07,  2.48it/s]\n",
      "Validation DataLoader 0:  82%|████████▏ | 79/96 [00:31<00:06,  2.48it/s]\n",
      "Validation DataLoader 0:  83%|████████▎ | 80/96 [00:32<00:06,  2.48it/s]\n",
      "Validation DataLoader 0:  84%|████████▍ | 81/96 [00:32<00:06,  2.48it/s]\n",
      "Validation DataLoader 0:  85%|████████▌ | 82/96 [00:33<00:05,  2.48it/s]\n",
      "Validation DataLoader 0:  86%|████████▋ | 83/96 [00:33<00:05,  2.48it/s]\n",
      "Validation DataLoader 0:  88%|████████▊ | 84/96 [00:33<00:04,  2.48it/s]\n",
      "Validation DataLoader 0:  89%|████████▊ | 85/96 [00:34<00:04,  2.48it/s]\n",
      "Validation DataLoader 0:  90%|████████▉ | 86/96 [00:34<00:04,  2.48it/s]\n",
      "Validation DataLoader 0:  91%|█████████ | 87/96 [00:35<00:03,  2.48it/s]\n",
      "Validation DataLoader 0:  92%|█████████▏| 88/96 [00:35<00:03,  2.48it/s]\n",
      "Validation DataLoader 0:  93%|█████████▎| 89/96 [00:35<00:02,  2.48it/s]\n",
      "Validation DataLoader 0:  94%|█████████▍| 90/96 [00:36<00:02,  2.48it/s]\n",
      "Validation DataLoader 0:  95%|█████████▍| 91/96 [00:36<00:02,  2.48it/s]\n",
      "Validation DataLoader 0:  96%|█████████▌| 92/96 [00:37<00:01,  2.48it/s]\n",
      "Validation DataLoader 0:  97%|█████████▋| 93/96 [00:37<00:01,  2.48it/s]\n",
      "Validation DataLoader 0:  98%|█████████▊| 94/96 [00:37<00:00,  2.48it/s]\n",
      "Validation DataLoader 0:  99%|█████████▉| 95/96 [00:38<00:00,  2.48it/s]\n",
      "Validation DataLoader 0: 100%|██████████| 96/96 [00:38<00:00,  2.49it/s]\n",
      "Epoch 0: 100%|██████████| 382/382 [06:18<00:00,  1.01it/s, v_num=n14m, train_loss_step=1.490, val_map=0.00714, val_map_50=0.0247, val_map_75=0.00169, val_map_small=0.000, val_map_medium=0.000221, val_map_large=0.00971, val_map_per_class=-1.00, train_loss_epoch=0.559]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 382/382 [06:20<00:00,  1.01it/s, v_num=n14m, train_loss_step=1.490, val_map=0.00714, val_map_50=0.0247, val_map_75=0.00169, val_map_small=0.000, val_map_medium=0.000221, val_map_large=0.00971, val_map_per_class=-1.00, train_loss_epoch=0.559]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 486/486 [04:09<00:00,  1.95it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_map          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.006323115434497595    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        test_map_50        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.02311818115413189    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        test_map_75        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.0015275644836947322   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_map_large       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.008088461123406887    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_map_medium      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">  0.00021259236382320523   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    test_map_per_class     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">           -1.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_map_small       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">  1.2709871270999429e-06   </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_map         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.006323115434497595   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       test_map_50       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.02311818115413189   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       test_map_75       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.0015275644836947322  \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_map_large      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.008088461123406887   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_map_medium     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m 0.00021259236382320523  \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   test_map_per_class    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m          -1.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_map_small      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m 1.2709871270999429e-06  \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_map': 0.006323115434497595,\n",
       "  'test_map_50': 0.02311818115413189,\n",
       "  'test_map_75': 0.0015275644836947322,\n",
       "  'test_map_small': 1.2709871270999429e-06,\n",
       "  'test_map_medium': 0.00021259236382320523,\n",
       "  'test_map_large': 0.008088461123406887,\n",
       "  'test_map_per_class': -1.0}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "data_module = PASCALDataModule(root_dir, batch_size=12)\n",
    "data_module.setup()\n",
    "train_loader = data_module.train_dataloader()\n",
    "val_loader = data_module.val_dataloader()\n",
    "test_loader = data_module.test_dataloader()\n",
    "model = LitDetectorModel(num_classes=20)  # Assuming 20 classes\n",
    "wandb_logger = WandbLogger(\n",
    "            entity='paibl',\n",
    "            project='active-learning',\n",
    "            name='test',\n",
    "            save_dir='/data2/eranario/intermediate_data/Active-Learning/PASCAL_logs/tests'\n",
    "        )\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=1, default_root_dir='/data2/eranario/intermediate_data/Active-Learning/PASCAL_logs/tests', log_every_n_steps=1, logger=wandb_logger)\n",
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "trainer.test(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
