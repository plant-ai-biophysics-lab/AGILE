Unloading openmpi/4.1.5
Unloading slurm/23.02.7
Loading slurm/23.02.7
Loading openmpi/4.1.5
INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.11 (you have 1.4.8). Upgrade using: pip install --upgrade albumentations
INFO: [rank: 0] Seed set to 42
INFO:lightning.fabric.utilities.seed:[rank: 0] Seed set to 42
INFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /tmp/tmptycw79jp
INFO:torch.distributed.nn.jit.instantiator:Writing /tmp/tmptycw79jp/_remote_module_non_scriptable.py
Downloading: "https://download.pytorch.org/models/resnet50-0676ba61.pth" to /home/eranario/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth
  0%|          | 0.00/97.8M [00:00<?, ?B/s]  8%|â–Š         | 7.58M/97.8M [00:00<00:01, 79.5MB/s] 17%|â–ˆâ–‹        | 17.0M/97.8M [00:00<00:00, 91.0MB/s] 27%|â–ˆâ–ˆâ–‹       | 26.0M/97.8M [00:00<00:00, 92.2MB/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 34.8M/97.8M [00:00<00:00, 92.3MB/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 44.0M/97.8M [00:00<00:00, 93.4MB/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 53.3M/97.8M [00:00<00:00, 94.7MB/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62.4M/97.8M [00:00<00:00, 88.3MB/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 71.0M/97.8M [00:00<00:00, 88.9MB/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 80.5M/97.8M [00:00<00:00, 92.1MB/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 89.3M/97.8M [00:01<00:00, 91.5MB/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97.8M/97.8M [00:01<00:00, 91.5MB/s]
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python pascal_detector.py --root_dir /group/jmearlesgrp/dat ...
INFO: GPU available: True (cuda), used: True
INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True
INFO: TPU available: False, using: 0 TPU cores
INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores
INFO: IPU available: False, using: 0 IPUs
INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs
INFO: HPU available: False, using: 0 HPUs
INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs
wandb: Currently logged in as: earlranario (paibl). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/wandb/run-20240711_212354-zm8myufq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pascal_random_cold-start_0711_chunk-1
wandb: â­ï¸ View project at https://wandb.ai/paibl/active-learning
wandb: ðŸš€ View run at https://wandb.ai/paibl/active-learning/runs/zm8myufq
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: 
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
INFO:lightning.pytorch.callbacks.model_summary:
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 24. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
INFO: `Trainer.fit` stopped: `max_epochs=50` reached.
INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=50` reached.
INFO: Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/zm8myufq/checkpoints/epoch=49-step=6000.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/zm8myufq/checkpoints/epoch=49-step=6000.ckpt
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/zm8myufq/checkpoints/epoch=49-step=6000.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/zm8myufq/checkpoints/epoch=49-step=6000.ckpt
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
wandb: - 0.007 MB of 0.007 MB uploadedwandb: \ 0.007 MB of 0.007 MB uploadedwandb: | 0.007 MB of 0.007 MB uploadedwandb: / 0.007 MB of 0.007 MB uploadedwandb: - 0.012 MB of 0.028 MB uploadedwandb: \ 0.012 MB of 0.028 MB uploadedwandb: | 0.028 MB of 0.028 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:             lr-Adam â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            test_map â–
wandb:         test_map_50 â–
wandb:         test_map_75 â–
wandb:      test_map_large â–
wandb:     test_map_medium â–
wandb:  test_map_per_class â–
wandb:      test_map_small â–
wandb:           test_size â–
wandb:    train_loss_epoch â–ˆâ–‡â–‡â–‡â–†â–†â–†â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     train_loss_step â–‡â–…â–‡â–†â–†â–†â–ˆâ–„â–„â–…â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–‚â–â–â–â–â–â–
wandb:          train_size â–
wandb: trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 50
wandb:             lr-Adam 0.0002
wandb:            test_map 0.1573
wandb:         test_map_50 0.3104
wandb:         test_map_75 0.13779
wandb:      test_map_large 0.19214
wandb:     test_map_medium 0.06592
wandb:  test_map_per_class -1.0
wandb:      test_map_small 0.01668
wandb:           test_size 5823
wandb:    train_loss_epoch 0.07357
wandb:     train_loss_step 0.05747
wandb:          train_size 2858
wandb: trainer/global_step 6000
wandb: 
wandb: ðŸš€ View run pascal_random_cold-start_0711_chunk-1 at: https://wandb.ai/paibl/active-learning/runs/zm8myufq
wandb: â­ï¸ View project at: https://wandb.ai/paibl/active-learning
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/wandb/run-20240711_212354-zm8myufq/logs
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python pascal_detector.py --root_dir /group/jmearlesgrp/dat ...
INFO: GPU available: True (cuda), used: True
INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True
INFO: TPU available: False, using: 0 TPU cores
INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores
INFO: IPU available: False, using: 0 IPUs
INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs
INFO: HPU available: False, using: 0 HPUs
INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/wandb/run-20240712_010428-ivrmlyqs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pascal_random_cold-start_0711_chunk-2
wandb: â­ï¸ View project at https://wandb.ai/paibl/active-learning
wandb: ðŸš€ View run at https://wandb.ai/paibl/active-learning/runs/ivrmlyqs
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: 
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
INFO:lightning.pytorch.callbacks.model_summary:
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 11. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
INFO: `Trainer.fit` stopped: `max_epochs=50` reached.
INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=50` reached.
INFO: Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/ivrmlyqs/checkpoints/epoch=48-step=5978.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/ivrmlyqs/checkpoints/epoch=48-step=5978.ckpt
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/ivrmlyqs/checkpoints/epoch=48-step=5978.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/ivrmlyqs/checkpoints/epoch=48-step=5978.ckpt
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
wandb: - 0.007 MB of 0.007 MB uploadedwandb: \ 0.007 MB of 0.007 MB uploadedwandb: | 0.007 MB of 0.022 MB uploadedwandb: / 0.007 MB of 0.022 MB uploadedwandb: - 0.022 MB of 0.022 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:             lr-Adam â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            test_map â–
wandb:         test_map_50 â–
wandb:         test_map_75 â–
wandb:      test_map_large â–
wandb:     test_map_medium â–
wandb:  test_map_per_class â–
wandb:      test_map_small â–
wandb:           test_size â–
wandb:    train_loss_epoch â–ˆâ–…â–…â–„â–„â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–
wandb:     train_loss_step â–†â–†â–‡â–†â–…â–…â–†â–‡â–‚â–†â–ˆâ–„â–ƒâ–„â–ƒâ–…â–ƒâ–ƒâ–…â–„â–‚â–„â–„â–„â–„â–…â–„â–†â–„â–„â–‚â–â–ƒâ–ƒâ–…â–â–„â–‚â–„â–
wandb:          train_size â–
wandb: trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 50
wandb:             lr-Adam 0.0002
wandb:            test_map 0.14365
wandb:         test_map_50 0.27646
wandb:         test_map_75 0.13384
wandb:      test_map_large 0.17602
wandb:     test_map_medium 0.0508
wandb:  test_map_per_class -1.0
wandb:      test_map_small 0.01362
wandb:           test_size 5823
wandb:    train_loss_epoch 0.04291
wandb:     train_loss_step 0.03305
wandb:          train_size 2915
wandb: trainer/global_step 6100
wandb: 
wandb: ðŸš€ View run pascal_random_cold-start_0711_chunk-2 at: https://wandb.ai/paibl/active-learning/runs/ivrmlyqs
wandb: â­ï¸ View project at: https://wandb.ai/paibl/active-learning
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/wandb/run-20240712_010428-ivrmlyqs/logs
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python pascal_detector.py --root_dir /group/jmearlesgrp/dat ...
INFO: GPU available: True (cuda), used: True
INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True
INFO: TPU available: False, using: 0 TPU cores
INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores
INFO: IPU available: False, using: 0 IPUs
INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs
INFO: HPU available: False, using: 0 HPUs
INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/wandb/run-20240712_043514-czzvvfws
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pascal_random_cold-start_0711_chunk-3
wandb: â­ï¸ View project at https://wandb.ai/paibl/active-learning
wandb: ðŸš€ View run at https://wandb.ai/paibl/active-learning/runs/czzvvfws
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: 
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
INFO:lightning.pytorch.callbacks.model_summary:
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 20. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
INFO: `Trainer.fit` stopped: `max_epochs=50` reached.
INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=50` reached.
INFO: Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/czzvvfws/checkpoints/epoch=41-step=5208.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/czzvvfws/checkpoints/epoch=41-step=5208.ckpt
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/czzvvfws/checkpoints/epoch=41-step=5208.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/czzvvfws/checkpoints/epoch=41-step=5208.ckpt
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
wandb: - 0.007 MB of 0.007 MB uploadedwandb: \ 0.007 MB of 0.023 MB uploadedwandb: | 0.007 MB of 0.023 MB uploadedwandb: / 0.007 MB of 0.023 MB uploadedwandb: - 0.007 MB of 0.023 MB uploadedwandb: \ 0.007 MB of 0.023 MB uploadedwandb: | 0.007 MB of 0.023 MB uploadedwandb: / 0.023 MB of 0.023 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:             lr-Adam â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            test_map â–
wandb:         test_map_50 â–
wandb:         test_map_75 â–
wandb:      test_map_large â–
wandb:     test_map_medium â–
wandb:  test_map_per_class â–
wandb:      test_map_small â–
wandb:           test_size â–
wandb:    train_loss_epoch â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–…â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb:     train_loss_step â–…â–â–„â–ƒâ–‚â–‚â–‚â–ƒâ–‚â–‚â–ƒâ–†â–ƒâ–‚â–„â–„â–ƒâ–ƒâ–‚â–‚â–ƒâ–ƒâ–‚â–ƒâ–‚â–ƒâ–ˆâ–„â–‚â–ƒâ–‚â–â–‚â–â–â–â–â–‚â–‚â–
wandb:          train_size â–
wandb: trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 50
wandb:             lr-Adam 0.0002
wandb:            test_map 0.13579
wandb:         test_map_50 0.25428
wandb:         test_map_75 0.13098
wandb:      test_map_large 0.16733
wandb:     test_map_medium 0.04919
wandb:  test_map_per_class -1.0
wandb:      test_map_small 0.02051
wandb:           test_size 5823
wandb:    train_loss_epoch 0.02885
wandb:     train_loss_step 0.02231
wandb:          train_size 2972
wandb: trainer/global_step 6200
wandb: 
wandb: ðŸš€ View run pascal_random_cold-start_0711_chunk-3 at: https://wandb.ai/paibl/active-learning/runs/czzvvfws
wandb: â­ï¸ View project at: https://wandb.ai/paibl/active-learning
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/wandb/run-20240712_043514-czzvvfws/logs
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python pascal_detector.py --root_dir /group/jmearlesgrp/dat ...
INFO: GPU available: True (cuda), used: True
INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True
INFO: TPU available: False, using: 0 TPU cores
INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores
INFO: IPU available: False, using: 0 IPUs
INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs
INFO: HPU available: False, using: 0 HPUs
INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/wandb/run-20240712_080810-yoz92rrh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pascal_random_cold-start_0711_chunk-4
wandb: â­ï¸ View project at https://wandb.ai/paibl/active-learning
wandb: ðŸš€ View run at https://wandb.ai/paibl/active-learning/runs/yoz92rrh
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: 
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
INFO:lightning.pytorch.callbacks.model_summary:
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 5. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
INFO: `Trainer.fit` stopped: `max_epochs=50` reached.
INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=50` reached.
INFO: Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/yoz92rrh/checkpoints/epoch=41-step=5334.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/yoz92rrh/checkpoints/epoch=41-step=5334.ckpt
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/yoz92rrh/checkpoints/epoch=41-step=5334.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/yoz92rrh/checkpoints/epoch=41-step=5334.ckpt
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
wandb: - 0.007 MB of 0.007 MB uploadedwandb: \ 0.007 MB of 0.023 MB uploadedwandb: | 0.007 MB of 0.023 MB uploadedwandb: / 0.007 MB of 0.023 MB uploadedwandb: - 0.023 MB of 0.023 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:             lr-Adam â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            test_map â–
wandb:         test_map_50 â–
wandb:         test_map_75 â–
wandb:      test_map_large â–
wandb:     test_map_medium â–
wandb:  test_map_per_class â–
wandb:      test_map_small â–
wandb:           test_size â–
wandb:    train_loss_epoch â–„â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–…â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆ
wandb:     train_loss_step â–ˆâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–ƒâ–‚â–„â–„â–„â–ƒâ–„â–‚â–‚â–ƒâ–‚â–ƒâ–ƒâ–â–ƒâ–ƒâ–„â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–‚â–‚â–„â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–„
wandb:          train_size â–
wandb: trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 50
wandb:             lr-Adam 0.0002
wandb:            test_map 0.12199
wandb:         test_map_50 0.22279
wandb:         test_map_75 0.12098
wandb:      test_map_large 0.15049
wandb:     test_map_medium 0.0456
wandb:  test_map_per_class -1.0
wandb:      test_map_small 0.01193
wandb:           test_size 5823
wandb:    train_loss_epoch 0.07531
wandb:     train_loss_step 0.05119
wandb:          train_size 3029
wandb: trainer/global_step 6350
wandb: 
wandb: ðŸš€ View run pascal_random_cold-start_0711_chunk-4 at: https://wandb.ai/paibl/active-learning/runs/yoz92rrh
wandb: â­ï¸ View project at: https://wandb.ai/paibl/active-learning
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/wandb/run-20240712_080810-yoz92rrh/logs
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python pascal_detector.py --root_dir /group/jmearlesgrp/dat ...
INFO: GPU available: True (cuda), used: True
INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True
INFO: TPU available: False, using: 0 TPU cores
INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores
INFO: IPU available: False, using: 0 IPUs
INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs
INFO: HPU available: False, using: 0 HPUs
INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/wandb/run-20240712_114622-1cnwmvwt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pascal_random_cold-start_0711_chunk-5
wandb: â­ï¸ View project at https://wandb.ai/paibl/active-learning
wandb: ðŸš€ View run at https://wandb.ai/paibl/active-learning/runs/1cnwmvwt
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: 
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
INFO:lightning.pytorch.callbacks.model_summary:
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 14. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
INFO: `Trainer.fit` stopped: `max_epochs=50` reached.
INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=50` reached.
INFO: Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/1cnwmvwt/checkpoints/epoch=42-step=5547.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/1cnwmvwt/checkpoints/epoch=42-step=5547.ckpt
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/1cnwmvwt/checkpoints/epoch=42-step=5547.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/1cnwmvwt/checkpoints/epoch=42-step=5547.ckpt
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
wandb: - 0.007 MB of 0.007 MB uploadedwandb: \ 0.007 MB of 0.023 MB uploadedwandb: | 0.009 MB of 0.023 MB uploadedwandb: / 0.021 MB of 0.023 MB uploadedwandb: - 0.021 MB of 0.023 MB uploadedwandb: \ 0.021 MB of 0.023 MB uploadedwandb: | 0.021 MB of 0.023 MB uploadedwandb: / 0.021 MB of 0.023 MB uploadedwandb: - 0.021 MB of 0.023 MB uploadedwandb: \ 0.021 MB of 0.023 MB uploadedwandb: | 0.021 MB of 0.023 MB uploadedwandb: / 0.021 MB of 0.023 MB uploadedwandb: - 0.021 MB of 0.023 MB uploadedwandb: \ 0.021 MB of 0.023 MB uploadedwandb: | 0.021 MB of 0.023 MB uploadedwandb: / 0.021 MB of 0.023 MB uploadedwandb: - 0.021 MB of 0.023 MB uploadedwandb: \ 0.021 MB of 0.023 MB uploadedwandb: | 0.021 MB of 0.023 MB uploadedwandb: / 0.021 MB of 0.023 MB uploadedwandb: - 0.021 MB of 0.023 MB uploadedwandb: \ 0.021 MB of 0.023 MB uploadedwandb: | 0.021 MB of 0.023 MB uploadedwandb: / 0.021 MB of 0.023 MB uploadedwandb: - 0.021 MB of 0.023 MB uploadedwandb: \ 0.023 MB of 0.023 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:             lr-Adam â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            test_map â–
wandb:         test_map_50 â–
wandb:         test_map_75 â–
wandb:      test_map_large â–
wandb:     test_map_medium â–
wandb:  test_map_per_class â–
wandb:      test_map_small â–
wandb:           test_size â–
wandb:    train_loss_epoch â–…â–‚â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–†â–ˆâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–
wandb:     train_loss_step â–„â–ƒâ–â–ƒâ–‚â–‚â–‚â–„â–ƒâ–ƒâ–ƒâ–…â–ƒâ–„â–ƒâ–‚â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–â–‚â–ƒâ–‚â–ˆâ–†â–…â–ƒâ–‚â–‚â–‚â–â–â–â–‚â–‚â–â–
wandb:          train_size â–
wandb: trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 50
wandb:             lr-Adam 0.0002
wandb:            test_map 0.11149
wandb:         test_map_50 0.20664
wandb:         test_map_75 0.11057
wandb:      test_map_large 0.13642
wandb:     test_map_medium 0.0446
wandb:  test_map_per_class -1.0
wandb:      test_map_small 0.01756
wandb:           test_size 5823
wandb:    train_loss_epoch 0.02258
wandb:     train_loss_step 0.02041
wandb:          train_size 3086
wandb: trainer/global_step 6450
wandb: 
wandb: ðŸš€ View run pascal_random_cold-start_0711_chunk-5 at: https://wandb.ai/paibl/active-learning/runs/1cnwmvwt
wandb: â­ï¸ View project at: https://wandb.ai/paibl/active-learning
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/wandb/run-20240712_114622-1cnwmvwt/logs
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python pascal_detector.py --root_dir /group/jmearlesgrp/dat ...
INFO: GPU available: True (cuda), used: True
INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True
INFO: TPU available: False, using: 0 TPU cores
INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores
INFO: IPU available: False, using: 0 IPUs
INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs
INFO: HPU available: False, using: 0 HPUs
INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/wandb/run-20240712_153335-ny4ggqpj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pascal_random_cold-start_0711_chunk-6
wandb: â­ï¸ View project at https://wandb.ai/paibl/active-learning
wandb: ðŸš€ View run at https://wandb.ai/paibl/active-learning/runs/ny4ggqpj
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: 
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
INFO:lightning.pytorch.callbacks.model_summary:
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 23. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
INFO: `Trainer.fit` stopped: `max_epochs=50` reached.
INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=50` reached.
INFO: Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/ny4ggqpj/checkpoints/epoch=40-step=5371.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/ny4ggqpj/checkpoints/epoch=40-step=5371.ckpt
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/ny4ggqpj/checkpoints/epoch=40-step=5371.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/ny4ggqpj/checkpoints/epoch=40-step=5371.ckpt
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
wandb: - 0.007 MB of 0.007 MB uploadedwandb: \ 0.007 MB of 0.023 MB uploadedwandb: | 0.007 MB of 0.023 MB uploadedwandb: / 0.023 MB of 0.023 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:             lr-Adam â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            test_map â–
wandb:         test_map_50 â–
wandb:         test_map_75 â–
wandb:      test_map_large â–
wandb:     test_map_medium â–
wandb:  test_map_per_class â–
wandb:      test_map_small â–
wandb:           test_size â–
wandb:    train_loss_epoch â–‡â–„â–‚â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–
wandb:     train_loss_step â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–ƒâ–‚â–‚â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–ƒâ–ƒâ–„â–ƒâ–‡â–ˆâ–ƒâ–‚â–„â–‚â–‚â–ƒâ–â–â–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–ƒâ–ƒâ–‚â–‚
wandb:          train_size â–
wandb: trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 50
wandb:             lr-Adam 0.0002
wandb:            test_map 0.10059
wandb:         test_map_50 0.18027
wandb:         test_map_75 0.10116
wandb:      test_map_large 0.12216
wandb:     test_map_medium 0.03551
wandb:  test_map_per_class -1.0
wandb:      test_map_small 0.01381
wandb:           test_size 5823
wandb:    train_loss_epoch 0.02223
wandb:     train_loss_step 0.01638
wandb:          train_size 3143
wandb: trainer/global_step 6550
wandb: 
wandb: ðŸš€ View run pascal_random_cold-start_0711_chunk-6 at: https://wandb.ai/paibl/active-learning/runs/ny4ggqpj
wandb: â­ï¸ View project at: https://wandb.ai/paibl/active-learning
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/wandb/run-20240712_153335-ny4ggqpj/logs
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python pascal_detector.py --root_dir /group/jmearlesgrp/dat ...
INFO: GPU available: True (cuda), used: True
INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True
INFO: TPU available: False, using: 0 TPU cores
INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores
INFO: IPU available: False, using: 0 IPUs
INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs
INFO: HPU available: False, using: 0 HPUs
INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/wandb/run-20240712_192128-ja0rm8rq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pascal_random_cold-start_0711_chunk-7
wandb: â­ï¸ View project at https://wandb.ai/paibl/active-learning
wandb: ðŸš€ View run at https://wandb.ai/paibl/active-learning/runs/ja0rm8rq
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: 
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
INFO:lightning.pytorch.callbacks.model_summary:
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 8. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
INFO: `Trainer.fit` stopped: `max_epochs=50` reached.
INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=50` reached.
INFO: Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/ja0rm8rq/checkpoints/epoch=45-step=6164.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/ja0rm8rq/checkpoints/epoch=45-step=6164.ckpt
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/ja0rm8rq/checkpoints/epoch=45-step=6164.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/ja0rm8rq/checkpoints/epoch=45-step=6164.ckpt
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
wandb: - 0.007 MB of 0.007 MB uploadedwandb: \ 0.007 MB of 0.023 MB uploadedwandb: | 0.007 MB of 0.023 MB uploadedwandb: / 0.007 MB of 0.023 MB uploadedwandb: - 0.007 MB of 0.023 MB uploadedwandb: \ 0.007 MB of 0.023 MB uploadedwandb: | 0.023 MB of 0.023 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:             lr-Adam â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            test_map â–
wandb:         test_map_50 â–
wandb:         test_map_75 â–
wandb:      test_map_large â–
wandb:     test_map_medium â–
wandb:  test_map_per_class â–
wandb:      test_map_small â–
wandb:           test_size â–
wandb:    train_loss_epoch â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–‚â–ˆâ–‚â–â–â–â–â–â–â–
wandb:     train_loss_step â–ƒâ–‚â–‚â–‚â–‚â–â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–ƒâ–‚â–‚â–â–â–‚â–â–â–‚â–â–â–‚â–‚â–ˆâ–‚â–‚â–‚â–‚â–â–â–‚â–â–‚
wandb:          train_size â–
wandb: trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 50
wandb:             lr-Adam 0.0002
wandb:            test_map 0.09984
wandb:         test_map_50 0.18476
wandb:         test_map_75 0.09532
wandb:      test_map_large 0.12028
wandb:     test_map_medium 0.0424
wandb:  test_map_per_class -1.0
wandb:      test_map_small 0.01109
wandb:           test_size 5823
wandb:    train_loss_epoch 0.01806
wandb:     train_loss_step 0.02319
wandb:          train_size 3200
wandb: trainer/global_step 6700
wandb: 
wandb: ðŸš€ View run pascal_random_cold-start_0711_chunk-7 at: https://wandb.ai/paibl/active-learning/runs/ja0rm8rq
wandb: â­ï¸ View project at: https://wandb.ai/paibl/active-learning
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/wandb/run-20240712_192128-ja0rm8rq/logs
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python pascal_detector.py --root_dir /group/jmearlesgrp/dat ...
INFO: GPU available: True (cuda), used: True
INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True
INFO: TPU available: False, using: 0 TPU cores
INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores
INFO: IPU available: False, using: 0 IPUs
INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs
INFO: HPU available: False, using: 0 HPUs
INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/wandb/run-20240712_231034-i5i87n8r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pascal_random_cold-start_0711_chunk-8
wandb: â­ï¸ View project at https://wandb.ai/paibl/active-learning
wandb: ðŸš€ View run at https://wandb.ai/paibl/active-learning/runs/i5i87n8r
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: 
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
INFO:lightning.pytorch.callbacks.model_summary:
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 17. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
INFO: `Trainer.fit` stopped: `max_epochs=50` reached.
INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=50` reached.
INFO: Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/i5i87n8r/checkpoints/epoch=30-step=4216.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/i5i87n8r/checkpoints/epoch=30-step=4216.ckpt
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/i5i87n8r/checkpoints/epoch=30-step=4216.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/i5i87n8r/checkpoints/epoch=30-step=4216.ckpt
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
wandb: - 0.007 MB of 0.007 MB uploadedwandb: \ 0.007 MB of 0.020 MB uploadedwandb: | 0.008 MB of 0.023 MB uploadedwandb: / 0.008 MB of 0.023 MB uploadedwandb: - 0.023 MB of 0.023 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:             lr-Adam â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            test_map â–
wandb:         test_map_50 â–
wandb:         test_map_75 â–
wandb:      test_map_large â–
wandb:     test_map_medium â–
wandb:  test_map_per_class â–
wandb:      test_map_small â–
wandb:           test_size â–
wandb:    train_loss_epoch â–…â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ˆâ–ˆâ–„â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚
wandb:     train_loss_step â–ˆâ–‚â–‚â–â–‚â–â–ƒâ–ƒâ–‚â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–…â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–â–‚â–â–ƒâ–ƒâ–â–ƒâ–‚â–‚â–â–‚
wandb:          train_size â–
wandb: trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 50
wandb:             lr-Adam 0.0002
wandb:            test_map 0.1066
wandb:         test_map_50 0.19617
wandb:         test_map_75 0.10443
wandb:      test_map_large 0.12906
wandb:     test_map_medium 0.04119
wandb:  test_map_per_class -1.0
wandb:      test_map_small 0.01311
wandb:           test_size 5823
wandb:    train_loss_epoch 0.0245
wandb:     train_loss_step 0.02248
wandb:          train_size 3257
wandb: trainer/global_step 6800
wandb: 
wandb: ðŸš€ View run pascal_random_cold-start_0711_chunk-8 at: https://wandb.ai/paibl/active-learning/runs/i5i87n8r
wandb: â­ï¸ View project at: https://wandb.ai/paibl/active-learning
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/wandb/run-20240712_231034-i5i87n8r/logs
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python pascal_detector.py --root_dir /group/jmearlesgrp/dat ...
INFO: GPU available: True (cuda), used: True
INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True
INFO: TPU available: False, using: 0 TPU cores
INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores
INFO: IPU available: False, using: 0 IPUs
INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs
INFO: HPU available: False, using: 0 HPUs
INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/wandb/run-20240713_030415-k4hckxqz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pascal_random_cold-start_0711_chunk-9
wandb: â­ï¸ View project at https://wandb.ai/paibl/active-learning
wandb: ðŸš€ View run at https://wandb.ai/paibl/active-learning/runs/k4hckxqz
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: 
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
INFO:lightning.pytorch.callbacks.model_summary:
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
INFO: `Trainer.fit` stopped: `max_epochs=50` reached.
INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=50` reached.
INFO: Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/k4hckxqz/checkpoints/epoch=46-step=6533.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/k4hckxqz/checkpoints/epoch=46-step=6533.ckpt
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/k4hckxqz/checkpoints/epoch=46-step=6533.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/k4hckxqz/checkpoints/epoch=46-step=6533.ckpt
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
wandb: - 0.007 MB of 0.023 MB uploadedwandb: \ 0.007 MB of 0.023 MB uploadedwandb: | 0.023 MB of 0.023 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:             lr-Adam â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            test_map â–
wandb:         test_map_50 â–
wandb:         test_map_75 â–
wandb:      test_map_large â–
wandb:     test_map_medium â–
wandb:  test_map_per_class â–
wandb:      test_map_small â–
wandb:           test_size â–
wandb:    train_loss_epoch â–„â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–…â–‡â–ˆâ–„â–‚â–â–â–â–â–â–â–
wandb:     train_loss_step â–„â–…â–ƒâ–‚â–‚â–â–‚â–ƒâ–‚â–ƒâ–‚â–â–…â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–„â–‚â–‡â–ˆâ–„â–â–ƒâ–ƒâ–‚â–â–‚â–‚â–‚â–
wandb:          train_size â–
wandb: trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 50
wandb:             lr-Adam 0.0002
wandb:            test_map 0.09496
wandb:         test_map_50 0.1731
wandb:         test_map_75 0.09369
wandb:      test_map_large 0.11523
wandb:     test_map_medium 0.03994
wandb:  test_map_per_class -1.0
wandb:      test_map_small 0.00908
wandb:           test_size 5823
wandb:    train_loss_epoch 0.01805
wandb:     train_loss_step 0.0587
wandb:          train_size 3314
wandb: trainer/global_step 6950
wandb: 
wandb: ðŸš€ View run pascal_random_cold-start_0711_chunk-9 at: https://wandb.ai/paibl/active-learning/runs/k4hckxqz
wandb: â­ï¸ View project at: https://wandb.ai/paibl/active-learning
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/wandb/run-20240713_030415-k4hckxqz/logs
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python pascal_detector.py --root_dir /group/jmearlesgrp/dat ...
INFO: GPU available: True (cuda), used: True
INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True
INFO: TPU available: False, using: 0 TPU cores
INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores
INFO: IPU available: False, using: 0 IPUs
INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs
INFO: HPU available: False, using: 0 HPUs
INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/wandb/run-20240713_070148-1y2q9bcn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pascal_random_cold-start_0711_chunk-10
wandb: â­ï¸ View project at https://wandb.ai/paibl/active-learning
wandb: ðŸš€ View run at https://wandb.ai/paibl/active-learning/runs/1y2q9bcn
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: 
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
INFO:lightning.pytorch.callbacks.model_summary:
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
INFO: `Trainer.fit` stopped: `max_epochs=50` reached.
INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=50` reached.
INFO: Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/1y2q9bcn/checkpoints/epoch=32-step=4653.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/1y2q9bcn/checkpoints/epoch=32-step=4653.ckpt
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/1y2q9bcn/checkpoints/epoch=32-step=4653.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/1y2q9bcn/checkpoints/epoch=32-step=4653.ckpt
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
wandb: - 0.007 MB of 0.007 MB uploadedwandb: \ 0.007 MB of 0.023 MB uploadedwandb: | 0.007 MB of 0.023 MB uploadedwandb: / 0.007 MB of 0.023 MB uploadedwandb: - 0.007 MB of 0.023 MB uploadedwandb: \ 0.007 MB of 0.023 MB uploadedwandb: | 0.007 MB of 0.023 MB uploadedwandb: / 0.023 MB of 0.023 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:             lr-Adam â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            test_map â–
wandb:         test_map_50 â–
wandb:         test_map_75 â–
wandb:      test_map_large â–
wandb:     test_map_medium â–
wandb:  test_map_per_class â–
wandb:      test_map_small â–
wandb:           test_size â–
wandb:    train_loss_epoch â–ˆâ–„â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–ˆâ–ˆâ–†â–ƒâ–‚â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:     train_loss_step â–ƒâ–‚â–‚â–ƒâ–â–…â–ƒâ–ƒâ–ƒâ–„â–ƒâ–…â–ƒâ–ƒâ–„â–…â–„â–ƒâ–ƒâ–„â–…â–ˆâ–…â–„â–ƒâ–ƒâ–ƒâ–ƒâ–…â–ƒâ–ƒâ–‚â–„â–ƒâ–‚â–ƒâ–†â–„â–„â–ƒ
wandb:          train_size â–
wandb: trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 50
wandb:             lr-Adam 0.0002
wandb:            test_map 0.09665
wandb:         test_map_50 0.17746
wandb:         test_map_75 0.09247
wandb:      test_map_large 0.11535
wandb:     test_map_medium 0.04237
wandb:  test_map_per_class -1.0
wandb:      test_map_small 0.01068
wandb:           test_size 5823
wandb:    train_loss_epoch 0.02058
wandb:     train_loss_step 0.02081
wandb:          train_size 3371
wandb: trainer/global_step 7050
wandb: 
wandb: ðŸš€ View run pascal_random_cold-start_0711_chunk-10 at: https://wandb.ai/paibl/active-learning/runs/1y2q9bcn
wandb: â­ï¸ View project at: https://wandb.ai/paibl/active-learning
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/wandb/run-20240713_070148-1y2q9bcn/logs
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python pascal_detector.py --root_dir /group/jmearlesgrp/dat ...
INFO: GPU available: True (cuda), used: True
INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True
INFO: TPU available: False, using: 0 TPU cores
INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores
INFO: IPU available: False, using: 0 IPUs
INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs
INFO: HPU available: False, using: 0 HPUs
INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/wandb/run-20240713_110151-dznokagf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pascal_random_cold-start_0711_chunk-11
wandb: â­ï¸ View project at https://wandb.ai/paibl/active-learning
wandb: ðŸš€ View run at https://wandb.ai/paibl/active-learning/runs/dznokagf
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: 
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
INFO:lightning.pytorch.callbacks.model_summary:
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
INFO: `Trainer.fit` stopped: `max_epochs=50` reached.
INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=50` reached.
INFO: Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/dznokagf/checkpoints/epoch=47-step=6864.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/dznokagf/checkpoints/epoch=47-step=6864.ckpt
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/dznokagf/checkpoints/epoch=47-step=6864.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/dznokagf/checkpoints/epoch=47-step=6864.ckpt
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
wandb: - 0.007 MB of 0.007 MB uploadedwandb: \ 0.007 MB of 0.023 MB uploadedwandb: | 0.010 MB of 0.023 MB uploadedwandb: / 0.010 MB of 0.023 MB uploadedwandb: - 0.010 MB of 0.023 MB uploadedwandb: \ 0.010 MB of 0.023 MB uploadedwandb: | 0.010 MB of 0.023 MB uploadedwandb: / 0.023 MB of 0.023 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:             lr-Adam â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            test_map â–
wandb:         test_map_50 â–
wandb:         test_map_75 â–
wandb:      test_map_large â–
wandb:     test_map_medium â–
wandb:  test_map_per_class â–
wandb:      test_map_small â–
wandb:           test_size â–
wandb:    train_loss_epoch â–„â–‚â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–„â–ˆâ–…â–ƒâ–â–â–â–â–â–â–â–
wandb:     train_loss_step â–ƒâ–‚â–â–‚â–‚â–‚â–â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‡â–„â–ˆâ–…â–ƒâ–‚â–‚â–â–‚â–ƒâ–ƒâ–‚â–
wandb:          train_size â–
wandb: trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 50
wandb:             lr-Adam 0.0002
wandb:            test_map 0.09205
wandb:         test_map_50 0.17308
wandb:         test_map_75 0.0884
wandb:      test_map_large 0.11161
wandb:     test_map_medium 0.03717
wandb:  test_map_per_class -1.0
wandb:      test_map_small 0.01201
wandb:           test_size 5823
wandb:    train_loss_epoch 0.01651
wandb:     train_loss_step 0.01199
wandb:          train_size 3428
wandb: trainer/global_step 7150
wandb: 
wandb: ðŸš€ View run pascal_random_cold-start_0711_chunk-11 at: https://wandb.ai/paibl/active-learning/runs/dznokagf
wandb: â­ï¸ View project at: https://wandb.ai/paibl/active-learning
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/wandb/run-20240713_110151-dznokagf/logs
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python pascal_detector.py --root_dir /group/jmearlesgrp/dat ...
INFO: GPU available: True (cuda), used: True
INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True
INFO: TPU available: False, using: 0 TPU cores
INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores
INFO: IPU available: False, using: 0 IPUs
INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs
INFO: HPU available: False, using: 0 HPUs
INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/wandb/run-20240713_150644-lg169022
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pascal_random_cold-start_0711_chunk-12
wandb: â­ï¸ View project at https://wandb.ai/paibl/active-learning
wandb: ðŸš€ View run at https://wandb.ai/paibl/active-learning/runs/lg169022
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: 
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
INFO:lightning.pytorch.callbacks.model_summary:
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
INFO: `Trainer.fit` stopped: `max_epochs=50` reached.
INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=50` reached.
INFO: Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/lg169022/checkpoints/epoch=21-step=3212.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/lg169022/checkpoints/epoch=21-step=3212.ckpt
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/lg169022/checkpoints/epoch=21-step=3212.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/lg169022/checkpoints/epoch=21-step=3212.ckpt
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
wandb: - 0.007 MB of 0.007 MB uploadedwandb: \ 0.007 MB of 0.024 MB uploadedwandb: | 0.007 MB of 0.024 MB uploadedwandb: / 0.024 MB of 0.024 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:             lr-Adam â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            test_map â–
wandb:         test_map_50 â–
wandb:         test_map_75 â–
wandb:      test_map_large â–
wandb:     test_map_medium â–
wandb:  test_map_per_class â–
wandb:      test_map_small â–
wandb:           test_size â–
wandb:    train_loss_epoch â–‡â–ƒâ–‚â–â–â–‚â–‚â–‚â–‚â–ˆâ–‡â–†â–‚â–‚â–â–â–â–â–â–â–â–â–‚â–â–„â–ƒâ–‚â–‚â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–
wandb:     train_loss_step â–ˆâ–â–„â–…â–â–‡â–â–…â–„â–‡â–ˆâ–†â–„â–„â–ƒâ–„â–„â–‚â–…â–ƒâ–…â–…â–„â–ƒâ–ˆâ–†â–„â–ƒâ–â–„â–ƒâ–†â–„â–…â–†â–‚â–…â–„â–ƒâ–ˆ
wandb:          train_size â–
wandb: trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 50
wandb:             lr-Adam 0.0002
wandb:            test_map 0.09446
wandb:         test_map_50 0.17587
wandb:         test_map_75 0.08883
wandb:      test_map_large 0.11397
wandb:     test_map_medium 0.03968
wandb:  test_map_per_class -1.0
wandb:      test_map_small 0.02107
wandb:           test_size 5823
wandb:    train_loss_epoch 0.01892
wandb:     train_loss_step 0.02945
wandb:          train_size 3485
wandb: trainer/global_step 7300
wandb: 
wandb: ðŸš€ View run pascal_random_cold-start_0711_chunk-12 at: https://wandb.ai/paibl/active-learning/runs/lg169022
wandb: â­ï¸ View project at: https://wandb.ai/paibl/active-learning
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/wandb/run-20240713_150644-lg169022/logs
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python pascal_detector.py --root_dir /group/jmearlesgrp/dat ...
INFO: GPU available: True (cuda), used: True
INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True
INFO: TPU available: False, using: 0 TPU cores
INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores
INFO: IPU available: False, using: 0 IPUs
INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs
INFO: HPU available: False, using: 0 HPUs
INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/wandb/run-20240713_191716-98d6wrvr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pascal_random_cold-start_0711_chunk-13
wandb: â­ï¸ View project at https://wandb.ai/paibl/active-learning
wandb: ðŸš€ View run at https://wandb.ai/paibl/active-learning/runs/98d6wrvr
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: 
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
INFO:lightning.pytorch.callbacks.model_summary:
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
INFO: `Trainer.fit` stopped: `max_epochs=50` reached.
INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=50` reached.
INFO: Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/98d6wrvr/checkpoints/epoch=48-step=7252.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/98d6wrvr/checkpoints/epoch=48-step=7252.ckpt
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/98d6wrvr/checkpoints/epoch=48-step=7252.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/98d6wrvr/checkpoints/epoch=48-step=7252.ckpt
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
wandb: - 0.007 MB of 0.007 MB uploadedwandb: \ 0.007 MB of 0.015 MB uploadedwandb: | 0.007 MB of 0.015 MB uploadedwandb: / 0.024 MB of 0.024 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:             lr-Adam â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            test_map â–
wandb:         test_map_50 â–
wandb:         test_map_75 â–
wandb:      test_map_large â–
wandb:     test_map_medium â–
wandb:  test_map_per_class â–
wandb:      test_map_small â–
wandb:           test_size â–
wandb:    train_loss_epoch â–ˆâ–„â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–†â–†â–ƒâ–‚â–‚â–â–â–â–â–â–â–„â–ˆâ–‡â–ƒâ–‚â–‚â–â–â–â–â–
wandb:     train_loss_step â–…â–„â–ƒâ–ƒâ–‚â–‚â–„â–„â–‚â–‚â–ƒâ–ƒâ–ƒâ–‚â–„â–ƒâ–ƒâ–ƒâ–ˆâ–„â–„â–ƒâ–„â–‚â–ƒâ–â–‚â–‚â–â–…â–…â–„â–ƒâ–„â–‚â–‚â–‚â–‚â–‚â–
wandb:          train_size â–
wandb: trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 50
wandb:             lr-Adam 0.0002
wandb:            test_map 0.09702
wandb:         test_map_50 0.18029
wandb:         test_map_75 0.09238
wandb:      test_map_large 0.11467
wandb:     test_map_medium 0.05173
wandb:  test_map_per_class -1.0
wandb:      test_map_small 0.01455
wandb:           test_size 5823
wandb:    train_loss_epoch 0.0171
wandb:     train_loss_step 0.01051
wandb:          train_size 3542
wandb: trainer/global_step 7400
wandb: 
wandb: ðŸš€ View run pascal_random_cold-start_0711_chunk-13 at: https://wandb.ai/paibl/active-learning/runs/98d6wrvr
wandb: â­ï¸ View project at: https://wandb.ai/paibl/active-learning
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/wandb/run-20240713_191716-98d6wrvr/logs
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python pascal_detector.py --root_dir /group/jmearlesgrp/dat ...
INFO: GPU available: True (cuda), used: True
INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True
INFO: TPU available: False, using: 0 TPU cores
INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores
INFO: IPU available: False, using: 0 IPUs
INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs
INFO: HPU available: False, using: 0 HPUs
INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/wandb/run-20240713_233025-wrogxrmd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pascal_random_cold-start_0711_chunk-14
wandb: â­ï¸ View project at https://wandb.ai/paibl/active-learning
wandb: ðŸš€ View run at https://wandb.ai/paibl/active-learning/runs/wrogxrmd
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: 
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
INFO:lightning.pytorch.callbacks.model_summary:
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
INFO: `Trainer.fit` stopped: `max_epochs=50` reached.
INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=50` reached.
INFO: Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/wrogxrmd/checkpoints/epoch=35-step=5400.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/wrogxrmd/checkpoints/epoch=35-step=5400.ckpt
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/wrogxrmd/checkpoints/epoch=35-step=5400.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/wrogxrmd/checkpoints/epoch=35-step=5400.ckpt
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
wandb: - 0.007 MB of 0.007 MB uploadedwandb: \ 0.007 MB of 0.024 MB uploadedwandb: | 0.007 MB of 0.024 MB uploadedwandb: / 0.007 MB of 0.024 MB uploadedwandb: - 0.024 MB of 0.024 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:             lr-Adam â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            test_map â–
wandb:         test_map_50 â–
wandb:         test_map_75 â–
wandb:      test_map_large â–
wandb:     test_map_medium â–
wandb:  test_map_per_class â–
wandb:      test_map_small â–
wandb:           test_size â–
wandb:    train_loss_epoch â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     train_loss_step â–„â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–„â–‚â–‚â–‚â–ƒâ–â–‚â–‚â–‚â–‚â–ˆâ–„â–„â–‚â–‚â–â–â–‚â–‚â–‚â–â–‚â–â–â–â–â–‚â–‚â–‚â–‚
wandb:          train_size â–
wandb: trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 50
wandb:             lr-Adam 0.0002
wandb:            test_map 0.09725
wandb:         test_map_50 0.17959
wandb:         test_map_75 0.09367
wandb:      test_map_large 0.11608
wandb:     test_map_medium 0.0419
wandb:  test_map_per_class -1.0
wandb:      test_map_small 0.01088
wandb:           test_size 5823
wandb:    train_loss_epoch 0.01733
wandb:     train_loss_step 0.02275
wandb:          train_size 3599
wandb: trainer/global_step 7500
wandb: 
wandb: ðŸš€ View run pascal_random_cold-start_0711_chunk-14 at: https://wandb.ai/paibl/active-learning/runs/wrogxrmd
wandb: â­ï¸ View project at: https://wandb.ai/paibl/active-learning
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/wandb/run-20240713_233025-wrogxrmd/logs
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python pascal_detector.py --root_dir /group/jmearlesgrp/dat ...
INFO: GPU available: True (cuda), used: True
INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True
INFO: TPU available: False, using: 0 TPU cores
INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores
INFO: IPU available: False, using: 0 IPUs
INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs
INFO: HPU available: False, using: 0 HPUs
INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/wandb/run-20240714_034638-z02ko4vk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pascal_random_cold-start_0711_chunk-15
wandb: â­ï¸ View project at https://wandb.ai/paibl/active-learning
wandb: ðŸš€ View run at https://wandb.ai/paibl/active-learning/runs/z02ko4vk
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: 
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
INFO:lightning.pytorch.callbacks.model_summary:
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
INFO: `Trainer.fit` stopped: `max_epochs=50` reached.
INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=50` reached.
INFO: Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/z02ko4vk/checkpoints/epoch=38-step=5967.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/z02ko4vk/checkpoints/epoch=38-step=5967.ckpt
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/z02ko4vk/checkpoints/epoch=38-step=5967.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/z02ko4vk/checkpoints/epoch=38-step=5967.ckpt
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
wandb: - 0.007 MB of 0.007 MB uploadedwandb: \ 0.007 MB of 0.024 MB uploadedwandb: | 0.007 MB of 0.024 MB uploadedwandb: / 0.024 MB of 0.024 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:             lr-Adam â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            test_map â–
wandb:         test_map_50 â–
wandb:         test_map_75 â–
wandb:      test_map_large â–
wandb:     test_map_medium â–
wandb:  test_map_per_class â–
wandb:      test_map_small â–
wandb:           test_size â–
wandb:    train_loss_epoch â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ƒâ–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     train_loss_step â–â–ƒâ–ƒâ–ƒâ–‚â–â–â–â–â–â–‚â–ƒâ–‚â–â–â–â–‚â–‚â–â–‚â–‚â–‚â–†â–ˆâ–‚â–‚â–â–â–‚â–‚â–â–â–â–‚â–ƒâ–â–‚â–‚â–‚â–
wandb:          train_size â–
wandb: trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 50
wandb:             lr-Adam 0.0002
wandb:            test_map 0.08756
wandb:         test_map_50 0.16692
wandb:         test_map_75 0.08183
wandb:      test_map_large 0.10618
wandb:     test_map_medium 0.03997
wandb:  test_map_per_class -1.0
wandb:      test_map_small 0.01242
wandb:           test_size 5823
wandb:    train_loss_epoch 0.0181
wandb:     train_loss_step 0.00696
wandb:          train_size 3656
wandb: trainer/global_step 7650
wandb: 
wandb: ðŸš€ View run pascal_random_cold-start_0711_chunk-15 at: https://wandb.ai/paibl/active-learning/runs/z02ko4vk
wandb: â­ï¸ View project at: https://wandb.ai/paibl/active-learning
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/wandb/run-20240714_034638-z02ko4vk/logs
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python pascal_detector.py --root_dir /group/jmearlesgrp/dat ...
INFO: GPU available: True (cuda), used: True
INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True
INFO: TPU available: False, using: 0 TPU cores
INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores
INFO: IPU available: False, using: 0 IPUs
INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs
INFO: HPU available: False, using: 0 HPUs
INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/wandb/run-20240714_080634-884mvjzs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pascal_random_cold-start_0711_chunk-16
wandb: â­ï¸ View project at https://wandb.ai/paibl/active-learning
wandb: ðŸš€ View run at https://wandb.ai/paibl/active-learning/runs/884mvjzs
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: 
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
INFO:lightning.pytorch.callbacks.model_summary:
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
wandb: Network error (ReadTimeout), entering retry loop.
INFO: `Trainer.fit` stopped: `max_epochs=50` reached.
INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=50` reached.
INFO: Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/884mvjzs/checkpoints/epoch=30-step=4805.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/884mvjzs/checkpoints/epoch=30-step=4805.ckpt
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/884mvjzs/checkpoints/epoch=30-step=4805.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/884mvjzs/checkpoints/epoch=30-step=4805.ckpt
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
wandb: - 0.007 MB of 0.007 MB uploadedwandb: \ 0.007 MB of 0.024 MB uploadedwandb: | 0.007 MB of 0.024 MB uploadedwandb: / 0.007 MB of 0.024 MB uploadedwandb: - 0.007 MB of 0.024 MB uploadedwandb: \ 0.024 MB of 0.024 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:             lr-Adam â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            test_map â–
wandb:         test_map_50 â–
wandb:         test_map_75 â–
wandb:      test_map_large â–
wandb:     test_map_medium â–
wandb:  test_map_per_class â–
wandb:      test_map_small â–
wandb:           test_size â–
wandb:    train_loss_epoch â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–ƒâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–ƒâ–‚â–â–
wandb:     train_loss_step â–ˆâ–‚â–†â–‚â–â–†â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–ƒâ–‚â–ƒâ–…â–…â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–â–â–â–‚â–„â–…â–ƒâ–‚â–â–‚
wandb:          train_size â–
wandb: trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 50
wandb:             lr-Adam 0.0002
wandb:            test_map 0.09166
wandb:         test_map_50 0.17285
wandb:         test_map_75 0.08858
wandb:      test_map_large 0.10943
wandb:     test_map_medium 0.03819
wandb:  test_map_per_class -1.0
wandb:      test_map_small 0.0126
wandb:           test_size 5823
wandb:    train_loss_epoch 0.01708
wandb:     train_loss_step 0.01411
wandb:          train_size 3713
wandb: trainer/global_step 7750
wandb: 
wandb: ðŸš€ View run pascal_random_cold-start_0711_chunk-16 at: https://wandb.ai/paibl/active-learning/runs/884mvjzs
wandb: â­ï¸ View project at: https://wandb.ai/paibl/active-learning
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/wandb/run-20240714_080634-884mvjzs/logs
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python pascal_detector.py --root_dir /group/jmearlesgrp/dat ...
INFO: GPU available: True (cuda), used: True
INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True
INFO: TPU available: False, using: 0 TPU cores
INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores
INFO: IPU available: False, using: 0 IPUs
INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs
INFO: HPU available: False, using: 0 HPUs
INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/wandb/run-20240714_123030-9qeuzu7e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pascal_random_cold-start_0711_chunk-17
wandb: â­ï¸ View project at https://wandb.ai/paibl/active-learning
wandb: ðŸš€ View run at https://wandb.ai/paibl/active-learning/runs/9qeuzu7e
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: 
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
INFO:lightning.pytorch.callbacks.model_summary:
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
INFO: `Trainer.fit` stopped: `max_epochs=50` reached.
INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=50` reached.
INFO: Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/9qeuzu7e/checkpoints/epoch=47-step=7584.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Restoring states from the checkpoint path at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/9qeuzu7e/checkpoints/epoch=47-step=7584.ckpt
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/9qeuzu7e/checkpoints/epoch=47-step=7584.ckpt
INFO:lightning.pytorch.utilities.rank_zero:Loaded model weights from the checkpoint at /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/active-learning/9qeuzu7e/checkpoints/epoch=47-step=7584.ckpt
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
wandb: - 0.007 MB of 0.007 MB uploadedwandb: \ 0.007 MB of 0.015 MB uploadedwandb: | 0.010 MB of 0.024 MB uploadedwandb: / 0.010 MB of 0.024 MB uploadedwandb: - 0.010 MB of 0.024 MB uploadedwandb: \ 0.024 MB of 0.024 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:             lr-Adam â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            test_map â–
wandb:         test_map_50 â–
wandb:         test_map_75 â–
wandb:      test_map_large â–
wandb:     test_map_medium â–
wandb:  test_map_per_class â–
wandb:      test_map_small â–
wandb:           test_size â–
wandb:    train_loss_epoch â–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–†â–ˆâ–ƒâ–‚â–â–â–â–â–â–â–â–â–â–
wandb:     train_loss_step â–…â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–ƒâ–„â–‚â–ƒâ–ƒâ–„â–ƒâ–ƒâ–‚â–„â–„â–„â–ƒâ–„â–…â–‚â–â–„â–ƒâ–‡â–‡â–ˆâ–„â–‚â–â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–ƒâ–‚â–„â–
wandb:          train_size â–
wandb: trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 50
wandb:             lr-Adam 0.0002
wandb:            test_map 0.08737
wandb:         test_map_50 0.16396
wandb:         test_map_75 0.08244
wandb:      test_map_large 0.10585
wandb:     test_map_medium 0.03383
wandb:  test_map_per_class -1.0
wandb:      test_map_small 0.00937
wandb:           test_size 5823
wandb:    train_loss_epoch 0.01695
wandb:     train_loss_step 0.00797
wandb:          train_size 3770
wandb: trainer/global_step 7900
wandb: 
wandb: ðŸš€ View run pascal_random_cold-start_0711_chunk-17 at: https://wandb.ai/paibl/active-learning/runs/9qeuzu7e
wandb: â­ï¸ View project at: https://wandb.ai/paibl/active-learning
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/wandb/run-20240714_123030-9qeuzu7e/logs
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python pascal_detector.py --root_dir /group/jmearlesgrp/dat ...
INFO: GPU available: True (cuda), used: True
INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True
INFO: TPU available: False, using: 0 TPU cores
INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores
INFO: IPU available: False, using: 0 IPUs
INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs
INFO: HPU available: False, using: 0 HPUs
INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /group/jmearlesgrp/intermediate_data/eranario/Active-Learning/PASCAL_logs/random/pascal_random_cold-start_0711/wandb/run-20240714_170138-y8zj2kwr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pascal_random_cold-start_0711_chunk-18
wandb: â­ï¸ View project at https://wandb.ai/paibl/active-learning
wandb: ðŸš€ View run at https://wandb.ai/paibl/active-learning/runs/y8zj2kwr
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: 
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
INFO:lightning.pytorch.callbacks.model_summary:
  | Name            | Type                 | Params
---------------------------------------------------------
0 | model           | CustomFasterRCNN     | 41.4 M
1 | fc6_dropout     | Dropout              | 0     
2 | fc7_dropout     | Dropout              | 0     
3 | val_map_metric  | MeanAveragePrecision | 0     
4 | test_map_metric | MeanAveragePrecision | 0     
---------------------------------------------------------
41.2 M    Trainable params
222 K     Non-trainable params
41.4 M    Total params
165.566   Total estimated model params size (MB)
/home/eranario/miniconda3/envs/lightning/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
slurmstepd: error: *** JOB 14448436 ON gpu-5-50 CANCELLED AT 2024-07-14T21:23:07 DUE TO TIME LIMIT ***
